# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¤šç»´åº¦LLMè¾…åŠ©è”é‚¦èšåˆå™¨ - åŸºäºçœŸå®æµé‡æ•°æ®çš„ä¸“å®¶è¯„åˆ†ç³»ç»Ÿ
"""

import json
import copy
import numpy as np
from typing import List, Dict, Optional
import torch
import logging
import hashlib
from .aggregation import FedAvgAggregator, LoRAFedAvgAggregator


class ExpertOutputConfig:
    """ä¸“å®¶è¾“å‡ºæ§åˆ¶é…ç½®"""

    def __init__(self,
                 show_expert_process: bool = True,
                 show_llm_response: bool = True,
                 show_detailed_analysis: bool = True,
                 show_aggregation_process: bool = True,
                 show_consensus_analysis: bool = True):
        self.show_expert_process = show_expert_process  # æ˜¾ç¤ºä¸“å®¶è¯„åˆ†è¿‡ç¨‹
        self.show_llm_response = show_llm_response  # æ˜¾ç¤ºLLMå®Œæ•´å“åº”
        self.show_detailed_analysis = show_detailed_analysis  # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
        self.show_aggregation_process = show_aggregation_process  # æ˜¾ç¤ºèšåˆè¿‡ç¨‹
        self.show_consensus_analysis = show_consensus_analysis  # æ˜¾ç¤ºä¸“å®¶ä¸€è‡´æ€§åˆ†æ

    @classmethod
    def minimal(cls):
        """æœ€å°è¾“å‡ºæ¨¡å¼ - åªæ˜¾ç¤ºå…³é”®ç»“æœ"""
        return cls(
            show_expert_process=True,
            show_llm_response=False,
            show_detailed_analysis=False,
            show_aggregation_process=False,
            show_consensus_analysis=False
        )

    @classmethod
    def detailed(cls):
        """è¯¦ç»†è¾“å‡ºæ¨¡å¼ - æ˜¾ç¤ºæ‰€æœ‰ä¿¡æ¯"""
        return cls(
            show_expert_process=True,
            show_llm_response=True,
            show_detailed_analysis=True,
            show_aggregation_process=True,
            show_consensus_analysis=True
        )


class EnhancedMultiDimensionalLLMAggregator:
    """å¢å¼ºç‰ˆå¤šç»´åº¦ä¸“å®¶è¯„åˆ†LLMèšåˆå™¨"""

    def __init__(self,
                 api_key: str = None,
                 model_name: str = "glm-4-flash-250414",
                 cache_rounds: int = 1,
                 min_confidence: float = 0.7,
                 is_lora_mode: bool = False,
                 dimensions: List[str] = None,
                 server_instance=None,
                 verbose: bool = True):  # æ·»åŠ è¯¦ç»†ç¨‹åº¦æ§åˆ¶
        """
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„ä¸“å®¶å†³ç­–è¿‡ç¨‹
        """
        self.api_key = api_key
        self.model_name = model_name
        self.cache_rounds = cache_rounds
        self.min_confidence = min_confidence
        self.is_lora_mode = is_lora_mode
        self.verbose = verbose  # æ§åˆ¶è¾“å‡ºè¯¦ç»†ç¨‹åº¦

        # å®ç”¨çš„ä¸“å®¶ç»´åº¦ï¼ˆåˆ é™¤ä¸šåŠ¡ä»·å€¼ä¸“å®¶ï¼‰
        self.dimensions = dimensions or [
            'model_performance',  # æ¨¡å‹æ€§èƒ½ä¸“å®¶
            'data_quality',  # æ•°æ®è´¨é‡ä¸“å®¶
            'spatial_distribution',  # ç©ºé—´åˆ†å¸ƒä¸“å®¶
            'temporal_stability',  # æ—¶åºç¨³å®šæ€§ä¸“å®¶
            'traffic_pattern'  # æµé‡æ¨¡å¼ä¸“å®¶ï¼ˆ5ä¸ªä¸“å®¶ï¼‰
        ]

        # åŠ¨æ€ç»´åº¦æƒé‡ï¼ˆæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´ï¼‰
        self.dimension_weights = self._initialize_dimension_weights()

        # ç¼“å­˜å’Œæ—¥å¿—
        self.score_cache = {}
        self.logger = logging.getLogger(__name__)
        self.server_instance = server_instance

        # è®¾ç½®å¤‡ç”¨èšåˆå™¨
        self.fallback_aggregator = LoRAFedAvgAggregator() if is_lora_mode else FedAvgAggregator()

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = None
        self._init_llm_client()

    def _initialize_dimension_weights(self):
        """åˆå§‹åŒ–ä¸“å®¶ç»´åº¦æƒé‡ï¼ˆ5ä¸ªä¸“å®¶ï¼‰"""
        return {
            'model_performance': 0.35,  # æ¨¡å‹è´¨é‡æœ€é‡è¦
            'data_quality': 0.25,  # æ•°æ®è´¨é‡æ¬¡ä¹‹
            'spatial_distribution': 0.15,  # åœ°ç†åˆ†å¸ƒ
            'temporal_stability': 0.15,  # æ—¶åºç¨³å®šæ€§
            'traffic_pattern': 0.10  # æµé‡æ¨¡å¼
        }

    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if self.api_key:
                from openai import OpenAI
                self.llm_client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://open.bigmodel.cn/api/paas/v4/"
                )
                self.logger.info("æˆåŠŸåˆå§‹åŒ–å¢å¼ºç‰ˆå¤šç»´åº¦LLMå®¢æˆ·ç«¯")
            else:
                self.logger.warning("æœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨èšåˆå™¨")
        except Exception as e:
            self.logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def aggregate(self, client_models: List[Dict], client_info: List[Dict] = None,
                  client_stats: List = None, round_idx: int = 0):
        """å¢å¼ºç‰ˆå¤šç»´åº¦è¯„åˆ†èšåˆ"""
        if not self.llm_client or not client_stats:
            print(f"\nâš ï¸  è½®æ¬¡ {round_idx}: æ— LLMå®¢æˆ·ç«¯æˆ–ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨èšåˆå™¨")
            return self.fallback_aggregator.aggregate(client_models)

        try:
            print(f"\nğŸš€ å¯åŠ¨å¢å¼ºç‰ˆå¤šç»´åº¦LLMèšåˆ - è½®æ¬¡ {round_idx}")

            # åŠ¨æ€è°ƒæ•´ç»´åº¦æƒé‡
            old_weights = self.dimension_weights.copy()
            self._adjust_dimension_weights(round_idx)

            # æ£€æŸ¥æƒé‡æ˜¯å¦æœ‰å˜åŒ–
            weight_changed = any(abs(old_weights[k] - self.dimension_weights[k]) > 0.001
                                 for k in self.dimension_weights.keys())
            if weight_changed:
                print(f"ğŸ“Š æƒé‡ç­–ç•¥å·²è°ƒæ•´ (è½®æ¬¡ {round_idx}):")
                for dim in self.dimension_weights.keys():
                    old_w = old_weights[dim]
                    new_w = self.dimension_weights[dim]
                    change = "â†—" if new_w > old_w else "â†˜" if new_w < old_w else "â†’"
                    print(f"   â€¢ {dim.replace('_', ' ').title()}: {old_w:.1%} {change} {new_w:.1%}")

            # è·å–å„ä¸“å®¶ç»´åº¦è¯„åˆ†
            dimension_scores = self._get_enhanced_dimension_scores(client_stats, round_idx)

            # è®¡ç®—åŠ æƒæ€»åˆ†
            final_scores = self._calculate_weighted_scores(dimension_scores)

            # è½¬æ¢ä¸ºèšåˆæƒé‡
            aggregation_weights = self._scores_to_weights(final_scores)

            print(f"\nğŸ¯ æœ€ç»ˆèšåˆå†³ç­–:")
            print(f"   ä½¿ç”¨å¢å¼ºç‰ˆå¤šç»´åº¦LLMæ™ºèƒ½æƒé‡è¿›è¡Œèšåˆ")
            print(f"   æƒé‡åˆ†å¸ƒ: {[f'{w:.3%}' for w in aggregation_weights]}")

            # æ‰§è¡Œèšåˆ
            result = self._weighted_aggregate(client_models, aggregation_weights)

            print(f"âœ… èšåˆå®Œæˆï¼")
            return result

        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆå¤šç»´åº¦èšåˆå¤±è´¥: {e}")
            print(f"ğŸ”„ å›é€€åˆ°å¤‡ç”¨èšåˆå™¨")
            return self.fallback_aggregator.aggregate(client_models)

    def _get_enhanced_dimension_scores(self, client_stats, round_idx):
        """è·å–å¢å¼ºç‰ˆå„ç»´åº¦è¯„åˆ†"""
        dimension_scores = {}

        if self.verbose:
            print(f"\n{'=' * 80}")
            print(f"ğŸ§  è½®æ¬¡ {round_idx} - ä¸“å®¶è¯„åˆ†ç³»ç»Ÿå†³ç­–è¿‡ç¨‹")
            print(f"{'=' * 80}")

            # æ˜¾ç¤ºå½“å‰ç»´åº¦æƒé‡
            print(f"ğŸ“Š å½“å‰ä¸“å®¶æƒé‡åˆ†é…:")
            for dim, weight in self.dimension_weights.items():
                print(f"   â€¢ {dim.replace('_', ' ').title()}: {weight:.1%}")
            print()

        for dimension in self.dimensions:
            try:
                if self.verbose:
                    print(f"ğŸ” {dimension.replace('_', ' ').title()}ä¸“å®¶æ­£åœ¨è¯„ä¼°...")

                # æ£€æŸ¥ç¼“å­˜
                cache_key = f"{dimension}_{round_idx}_{self._get_stats_hash(client_stats)}"
                if cache_key in self.score_cache:
                    dimension_scores[dimension] = self.score_cache[cache_key]
                    if self.verbose:
                        print(f"   âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {[f'{s:.2f}' for s in dimension_scores[dimension]]}")
                    continue

                # è°ƒç”¨å¯¹åº”çš„ä¸“å®¶è¯„åˆ†å‡½æ•°
                if self.verbose:
                    print(f"   ğŸ¤– è°ƒç”¨LLMè¿›è¡Œä¸“ä¸šè¯„åˆ†...")
                scores = getattr(self, f'_score_{dimension}')(client_stats, round_idx)
                dimension_scores[dimension] = scores

                # ç¼“å­˜ç»“æœ
                self.score_cache[cache_key] = scores

                # æ˜¾ç¤ºè¯„åˆ†ç»“æœå’Œåˆ†æ
                if self.verbose:
                    self._print_expert_analysis(dimension, scores, client_stats)

            except Exception as e:
                if self.verbose:
                    print(f"   âŒ {dimension}ç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                    print(f"   ğŸ”„ ä½¿ç”¨é»˜è®¤è¯„åˆ†: {[1.0] * len(client_stats)}")
                else:
                    self.logger.warning(f"{dimension}ç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤è¯„åˆ†ä½œä¸ºå¤‡ç”¨
                dimension_scores[dimension] = [1.0] * len(client_stats)

        # æ˜¾ç¤ºç»¼åˆåˆ†æ
        if self.verbose:
            self._print_comprehensive_analysis(dimension_scores, client_stats, round_idx)

        return dimension_scores

    def _score_model_performance(self, client_stats, round_idx):
        """æ¨¡å‹æ€§èƒ½ä¸“å®¶è¯„åˆ†"""
        performance_data = []
        for i, stats in enumerate(client_stats):
            # ç»“åˆå¤šä¸ªæ€§èƒ½æŒ‡æ ‡
            loss = stats.loss
            trend_info = getattr(stats, 'trend_info', {})

            performance_data.append({
                'client': i,
                'loss': f"{loss:.4f}",
                'trend': trend_info.get('description', 'unknown'),
                'improvement_rate': f"{trend_info.get('improvement_rate', 0):.1f}%",
                'participation': getattr(stats, 'participation_count', 1)
            })

        prompt = f"""ä½ æ˜¯æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸“å®¶ã€‚è¯·ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„æ¨¡å‹è®­ç»ƒæ€§èƒ½ç»™å‡ºè¯„åˆ†ã€‚

æ€§èƒ½æ•°æ®ï¼š
{json.dumps(performance_data, indent=2, ensure_ascii=False)}

è¯„åˆ†åŸåˆ™ï¼š
1. **æŸå¤±å€¼æƒé‡70%**: æŸå¤±è¶Šä½è¯„åˆ†è¶Šé«˜ï¼Œä½†è¦è­¦æƒ•è¿‡æ‹Ÿåˆ
2. **æ”¹è¿›è¶‹åŠ¿æƒé‡20%**: strongly_improving(1.5x), improving(1.2x), stable(1.0x), deteriorating(0.7x)
3. **å‚ä¸ç¨³å®šæ€§æƒé‡10%**: å‚ä¸è½®æ•°å¤šçš„å®¢æˆ·ç«¯æ›´å¯é 

ç‰¹æ®Šè€ƒè™‘ï¼š
- æŸå¤±å¼‚å¸¸ä½(<0.1)å¯èƒ½è¿‡æ‹Ÿåˆï¼Œé€‚å½“æƒ©ç½š
- æ”¹è¿›ç‡>20%çš„å®¢æˆ·ç«¯ç»™äºˆå¥–åŠ±
- æ–°å®¢æˆ·ç«¯(å‚ä¸<3è½®)é€‚å½“ä¿å®ˆè¯„åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.3-2.0ï¼Œç›´æ¥è¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_data_quality(self, client_stats, round_idx):
        """æ•°æ®è´¨é‡ä¸“å®¶è¯„åˆ†"""
        quality_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats
            quality_data.append({
                'client': i,
                'data_points': traffic.get('data_points', 0),
                'cv': f"{traffic.get('coefficient_of_variation', 0):.3f}",
                'iqr': f"{traffic.get('iqr', 0):.1f}",
                'trend_stability': traffic.get('trend', 'unknown'),
                'range_ratio': f"{traffic.get('max', 1) / max(traffic.get('min', 1), 0.1):.1f}"
            })

        prompt = f"""ä½ æ˜¯æ•°æ®è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºçœŸå®æµé‡ç»Ÿè®¡ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„æ•°æ®è´¨é‡è¯„åˆ†ã€‚

æ•°æ®è´¨é‡æŒ‡æ ‡ï¼š
{json.dumps(quality_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **æ•°æ®å……è¶³æ€§(30%)**: data_pointsè¶Šå¤šè¶Šå¥½ï¼Œ<100æƒ©ç½šï¼Œ>500å¥–åŠ±
2. **å˜å¼‚ç³»æ•°(40%)**: cv<0.5(ä¼˜ç§€1.5x), 0.5-1.0(è‰¯å¥½1.0x), >1.0(è¾ƒå·®0.7x)
3. **åˆ†å¸ƒåˆç†æ€§(20%)**: iqré€‚ä¸­æœ€å¥½ï¼Œè¿‡å¤§æˆ–è¿‡å°éƒ½ä¸å¥½
4. **è¶‹åŠ¿ç¨³å®šæ€§(10%)**: stable>increasing>decreasing

ç‰¹æ®Šå¤„ç†ï¼š
- cv>2.0è¡¨ç¤ºæä¸ç¨³å®šï¼Œä¸¥é‡æƒ©ç½š(0.4x)
- range_ratio>100è¡¨ç¤ºæ•°æ®å¼‚å¸¸ï¼Œæƒ©ç½š
- trend='stable'ä¸”cv<0.3ç»™äºˆå¥–åŠ±

è¯„åˆ†èŒƒå›´ï¼š0.2-1.8ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_spatial_distribution(self, client_stats, round_idx):
        """ç©ºé—´åˆ†å¸ƒä¸“å®¶è¯„åˆ†"""
        spatial_data = []
        locations = []

        for i, stats in enumerate(client_stats):
            lng, lat = stats.coordinates['lng'], stats.coordinates['lat']
            locations.append((lng, lat))
            spatial_data.append({
                'client': i,
                'lng': f"{lng:.3f}",
                'lat': f"{lat:.3f}",
                'location_type': self._classify_location_type(lng, lat)
            })

        # è®¡ç®—ç©ºé—´åˆ†æ•£åº¦
        diversity_score = self._calculate_spatial_diversity(locations)

        prompt = f"""ä½ æ˜¯ç©ºé—´åˆ†å¸ƒä¸“å®¶ã€‚è¯·ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„åœ°ç†ä½ç½®ä»£è¡¨æ€§è¯„åˆ†ã€‚

ä½ç½®ä¿¡æ¯ï¼š
{json.dumps(spatial_data, indent=2, ensure_ascii=False)}

ç©ºé—´åˆ†æ•£åº¦è¯„åˆ†: {diversity_score:.3f} (0-1, è¶Šé«˜è¶Šåˆ†æ•£)

è¯„åˆ†åŸåˆ™ï¼š
1. **åˆ†æ•£æ€§å¥–åŠ±(50%)**: åŸºäºæ•´ä½“ç©ºé—´åˆ†æ•£åº¦ï¼Œè¶Šåˆ†æ•£è¶Šå¥½
2. **è¾¹ç¼˜ä»·å€¼(30%)**: ä½äºè¾¹ç¼˜çš„åŸºç«™æœ‰ç‹¬ç‰¹ä»·å€¼
3. **è¦†ç›–å‡è¡¡(20%)**: é¿å…æŸä¸ªåŒºåŸŸè¿‡åº¦é›†ä¸­

ç‰¹æ®Šè€ƒè™‘ï¼š
- åˆ†æ•£åº¦>0.7æ—¶æ‰€æœ‰åŸºç«™è·å¾—å¥–åŠ±
- ä½äºåœ°ç†è¾¹ç•Œçš„åŸºç«™é¢å¤–åŠ åˆ†
- è¿‡åº¦èšé›†çš„åŸºç«™ç¾¤ç»„å†…è¯„åˆ†é€’å‡

è¯„åˆ†èŒƒå›´ï¼š0.6-1.4ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_temporal_stability(self, client_stats, round_idx):
        """æ—¶åºç¨³å®šæ€§ä¸“å®¶è¯„åˆ†"""
        temporal_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats
            temporal_data.append({
                'client': i,
                'trend': traffic.get('trend', 'unknown'),
                'trend_slope': f"{traffic.get('trend_slope', 0):.4f}",
                'recent_vs_avg': f"{traffic.get('recent_mean', 0) / max(traffic.get('mean', 1), 0.1):.2f}",
                'cv': f"{traffic.get('coefficient_of_variation', 0):.3f}"
            })

        prompt = f"""ä½ æ˜¯æ—¶åºç¨³å®šæ€§ä¸“å®¶ã€‚è¯·è¯„ä¼°{len(client_stats)}ä¸ªåŸºç«™çš„æ—¶é—´åºåˆ—ç¨³å®šæ€§ã€‚

æ—¶åºç‰¹å¾ï¼š
{json.dumps(temporal_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **è¶‹åŠ¿ç¨³å®šæ€§(40%)**: stable(1.2x), increasing(1.0x), decreasing(0.8x)
2. **æ–œç‡åˆç†æ€§(30%)**: |slope|<0.01(å¥½), 0.01-0.05(ä¸€èˆ¬), >0.05(å·®)
3. **æœ€è¿‘æœŸä¸€è‡´æ€§(20%)**: recent_vs_avgåœ¨0.8-1.2ä¸ºå¥½
4. **æ³¢åŠ¨æ§åˆ¶(10%)**: cv<0.5ä¸ºç¨³å®š

æ—¶åºè´¨é‡è¯„çº§ï¼š
- ä¼˜ç§€: stable + low_cv + ä¸€è‡´æ€§å¥½ â†’ 1.3-1.6åˆ†
- è‰¯å¥½: è½»å¾®æ³¢åŠ¨ä½†æ•´ä½“ç¨³å®š â†’ 1.0-1.3åˆ†  
- ä¸€èˆ¬: æœ‰æ˜æ˜¾è¶‹åŠ¿ä½†å¯æ§ â†’ 0.7-1.0åˆ†
- è¾ƒå·®: é«˜æ³¢åŠ¨æˆ–å¼‚å¸¸è¶‹åŠ¿ â†’ 0.4-0.7åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.4-1.6ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_traffic_pattern(self, client_stats, round_idx):
        """æµé‡æ¨¡å¼ä¸“å®¶è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆ - åŸºäºçœŸå®ç»Ÿè®¡ç‰¹å¾ï¼‰"""
        pattern_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats

            # å¢å¼ºçš„æµé‡æ¨¡å¼åˆ†æ
            mean_traffic = traffic.get('mean', 0)
            cv = traffic.get('coefficient_of_variation', 0)
            trend = traffic.get('trend', 'unknown')
            trend_slope = abs(traffic.get('trend_slope', 0))
            iqr = traffic.get('iqr', 0)

            pattern_data.append({
                'client': i,
                'avg_traffic': f"{mean_traffic:.1f}",
                'traffic_level': self._classify_traffic_level(mean_traffic),
                'stability': 'stable' if cv < 0.5 else 'moderate' if cv < 1.0 else 'unstable',
                'trend_strength': 'weak' if trend_slope < 0.01 else 'moderate' if trend_slope < 0.05 else 'strong',
                'trend_direction': trend,
                'variability': f"{cv:.3f}",
                'data_spread': f"{iqr:.1f}",
                'pattern_quality': self._assess_pattern_quality(traffic)
            })

        prompt = f"""ä½ æ˜¯æµé‡æ¨¡å¼ä¸“å®¶ã€‚è¯·åŸºäºçœŸå®æµé‡ç»Ÿè®¡è¯„ä¼°{len(client_stats)}ä¸ªåŸºç«™çš„æµé‡æ¨¡å¼è´¨é‡ã€‚

è¯¦ç»†æµé‡æ¨¡å¼åˆ†æï¼š
{json.dumps(pattern_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **æµé‡ç¨³å®šæ€§(40%)**: 
   - stable(cv<0.5): 1.3x - æ•°æ®å¯é ï¼Œé€‚åˆè®­ç»ƒ
   - moderate(0.5â‰¤cv<1.0): 1.0x - ä¸­ç­‰è´¨é‡
   - unstable(cvâ‰¥1.0): 0.7x - ä¸ç¨³å®šï¼Œå½±å“å­¦ä¹ 

2. **æµé‡æ°´å¹³åˆç†æ€§(25%)**: 
   - ä¸­ç­‰æµé‡(50-500): 1.2x - æœ€ä½³è®­ç»ƒåŒºé—´
   - é«˜æµé‡(>500): 1.1x - é‡è¦ä½†å¯èƒ½æœ‰å™ªå£°
   - ä½æµé‡(<50): 0.9x - ä¿¡å·è¾ƒå¼±

3. **è¶‹åŠ¿ç‰¹å¾(20%)**: 
   - weak trend + stable: 1.2x - ç†æƒ³çš„å¹³ç¨³æ¨¡å¼
   - moderate trend: 1.0x - æ­£å¸¸å˜åŒ–
   - strong trend: 0.8x - å¯èƒ½å­˜åœ¨å¼‚å¸¸

4. **æ•°æ®åˆ†å¸ƒè´¨é‡(15%)**: 
   - é€‚ä¸­çš„IQR(10-100): 1.1x
   - è¿‡å°æˆ–è¿‡å¤§çš„IQR: 0.9x

ç‰¹æ®ŠåŠ åˆ†é¡¹ï¼š
- stable + moderate traffic + weak trend: +0.2åˆ†
- æ•°æ®è´¨é‡è¯„çº§ä¸º'good'çš„åŸºç«™: +0.1åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.4-1.8ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    # è¾…åŠ©å‡½æ•°
    def _classify_location_type(self, lng, lat):
        """åˆ†ç±»ä½ç½®ç±»å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…åœ°ç†ä¿¡æ¯è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ†ç±»
        if abs(lng - 116.3) < 0.1 and abs(lat - 39.9) < 0.1:
            return "core_business"
        elif abs(lng - 116.3) < 0.5 and abs(lat - 39.9) < 0.5:
            return "urban_area"
        else:
            return "suburban"

    def _calculate_spatial_diversity(self, locations):
        """è®¡ç®—ç©ºé—´åˆ†æ•£åº¦"""
        if len(locations) < 2:
            return 0.5

        distances = []
        for i in range(len(locations)):
            for j in range(i + 1, len(locations)):
                lng1, lat1 = locations[i]
                lng2, lat2 = locations[j]
                dist = ((lng1 - lng2) ** 2 + (lat1 - lat2) ** 2) ** 0.5
                distances.append(dist)

        avg_distance = np.mean(distances)
        max_distance = max(distances)

        # å½’ä¸€åŒ–åˆ°0-1
        diversity = min(1.0, avg_distance / (max_distance + 1e-6))
        return diversity

    def _classify_traffic_level(self, mean_traffic):
        """åˆ†ç±»æµé‡æ°´å¹³"""
        if mean_traffic > 300:
            return "high"
        elif mean_traffic > 100:
            return "medium"
        else:
            return "low"

    def _classify_distribution_type(self, traffic_stats):
        """åˆ†ç±»æµé‡åˆ†å¸ƒç±»å‹"""
        cv = traffic_stats.get('coefficient_of_variation', 0)
        if cv < 0.3:
            return "stable"
        elif cv < 0.8:
            return "moderate_variation"
        else:
            return "high_variation"

    def _assess_pattern_quality(self, traffic_stats):
        """è¯„ä¼°æµé‡æ¨¡å¼è´¨é‡ï¼ˆæ–°å¢ï¼‰"""
        cv = traffic_stats.get('coefficient_of_variation', 0)
        mean = traffic_stats.get('mean', 0)
        trend = traffic_stats.get('trend', 'unknown')

        # ç»¼åˆè¯„ä¼°æ¨¡å¼è´¨é‡
        quality_score = 0

        # ç¨³å®šæ€§è¯„åˆ†
        if cv < 0.3:
            quality_score += 3  # éå¸¸ç¨³å®š
        elif cv < 0.7:
            quality_score += 2  # è¾ƒç¨³å®š
        else:
            quality_score += 1  # ä¸ç¨³å®š

        # æµé‡æ°´å¹³è¯„åˆ†
        if 50 <= mean <= 500:
            quality_score += 2  # åˆç†èŒƒå›´
        else:
            quality_score += 1  # åç¦»ç†æƒ³èŒƒå›´

        # è¶‹åŠ¿è¯„åˆ†
        if trend == 'stable':
            quality_score += 2
        elif trend in ['increasing', 'decreasing']:
            quality_score += 1

        # è½¬æ¢ä¸ºè´¨é‡ç­‰çº§
        if quality_score >= 6:
            return 'excellent'
        elif quality_score >= 5:
            return 'good'
        elif quality_score >= 3:
            return 'fair'
        else:
            return 'poor'

    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ–¹æ³•...
    def _calculate_weighted_scores(self, dimension_scores):
        """è®¡ç®—åŠ æƒæ€»åˆ†"""
        num_clients = len(next(iter(dimension_scores.values())))
        final_scores = [0.0] * num_clients

        total_weight = 0.0
        for dimension, scores in dimension_scores.items():
            weight = self.dimension_weights.get(dimension, 1.0 / len(self.dimensions))
            total_weight += weight

            for i in range(num_clients):
                final_scores[i] += scores[i] * weight

        # å½’ä¸€åŒ–
        if total_weight != 1.0:
            final_scores = [score / total_weight for score in final_scores]

        self.logger.info(f"åŠ æƒæ€»åˆ†: {[f'{s:.3f}' for s in final_scores]}")
        return final_scores

    def _scores_to_weights(self, scores):
        """å°†è¯„åˆ†è½¬æ¢ä¸ºèšåˆæƒé‡"""
        scores_array = np.array(scores)
        temperature = 1.2  # è°ƒæ•´æ¸©åº¦å‚æ•°
        scores_array = scores_array / temperature
        exp_scores = np.exp(scores_array - np.max(scores_array))
        weights = exp_scores / np.sum(exp_scores)

        self.logger.info(f"Softmaxæƒé‡: {[f'{w:.3f}' for w in weights]}")
        return weights.tolist()

    def _weighted_aggregate(self, client_models: List[Dict], weights: List[float]) -> Dict:
        """æ‰§è¡ŒåŠ æƒèšåˆå¹¶æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹"""
        if not client_models:
            raise ValueError("å®¢æˆ·ç«¯æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        print(f"\nâš™ï¸  æ‰§è¡ŒåŠ æƒæ¨¡å‹èšåˆ:")
        print(f"   å‚ä¸èšåˆçš„å®¢æˆ·ç«¯æ•°é‡: {len(client_models)}")
        print(f"   èšåˆæƒé‡: {[f'{w:.3%}' for w in weights]}")

        # æ˜¾ç¤ºæƒé‡åˆ†å¸ƒç»Ÿè®¡
        max_weight = max(weights)
        min_weight = min(weights)
        avg_weight = sum(weights) / len(weights)

        print(f"   æƒé‡ç»Ÿè®¡: æœ€å¤§={max_weight:.3%}, æœ€å°={min_weight:.3%}, å¹³å‡={avg_weight:.3%}")

        aggregated_model = {}
        total_params = 0

        # ç»Ÿè®¡å‚æ•°ä¿¡æ¯
        param_info = {}
        for key in client_models[0].keys():
            param_shape = client_models[0][key].shape
            param_count = client_models[0][key].numel()
            param_info[key] = {'shape': param_shape, 'count': param_count}
            total_params += param_count

        print(f"   æ¨¡å‹å‚æ•°ç»Ÿè®¡: æ€»è®¡ {total_params:,} ä¸ªå‚æ•°")

        if self.is_lora_mode:
            lora_params = sum(
                1 for key in param_info.keys() if any(lora_key in key for lora_key in ['lora_A', 'lora_B']))
            print(f"   LoRAæ¨¡å¼: {lora_params} ä¸ªLoRAå‚æ•°æ¨¡å—")

        # æ‰§è¡Œèšåˆ
        print(f"   ğŸ”„ æ­£åœ¨èšåˆæ¨¡å‹å‚æ•°...")

        for key in client_models[0].keys():
            aggregated_model[key] = torch.zeros_like(client_models[0][key])
            for i, client_model in enumerate(client_models):
                if key in client_model:
                    aggregated_model[key] += client_model[key] * weights[i]

        print(f"   âœ… å‚æ•°èšåˆå®Œæˆ")

        # æ˜¾ç¤ºèšåˆæ•ˆæœåˆ†æ
        print(f"   ğŸ“Š èšåˆæ•ˆæœåˆ†æ:")
        print(f"      â€¢ å‚æ•°æ›´æ–°å®Œæˆ: {len(aggregated_model)} ä¸ªæ¨¡å—")

        if self.is_lora_mode:
            print(f"      â€¢ LoRAå‚æ•°æ€»é‡: {total_params:,}")
            print(f"      â€¢ é€šä¿¡æ•ˆç‡: 99%+ (ä»…ä¼ è¾“LoRAå‚æ•°)")

        # æƒé‡åˆ†å¸ƒåˆ†æ
        weight_entropy = -sum(w * np.log(w + 1e-10) for w in weights)  # ä¿¡æ¯ç†µ
        max_entropy = np.log(len(weights))  # æœ€å¤§ç†µï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        diversity_ratio = weight_entropy / max_entropy

        print(f"      â€¢ æƒé‡å¤šæ ·æ€§: {diversity_ratio:.2%} (100%ä¸ºå®Œå…¨å‡åŒ€)")

        if diversity_ratio > 0.9:
            print(f"      â€¢ å†³ç­–ç±»å‹: æ°‘ä¸»åŒ–èšåˆ (æƒé‡åˆ†å¸ƒå‡åŒ€)")
        elif diversity_ratio > 0.7:
            print(f"      â€¢ å†³ç­–ç±»å‹: å¹³è¡¡èšåˆ (æƒé‡é€‚åº¦é›†ä¸­)")
        else:
            print(f"      â€¢ å†³ç­–ç±»å‹: ç²¾è‹±èšåˆ (æƒé‡é«˜åº¦é›†ä¸­)")

        return aggregated_model

    def _parse_scores(self, response, expected_length, default_score=1.0):
        """è§£æLLMè¿”å›çš„è¯„åˆ†"""
        try:
            import re
            array_match = re.search(r'\[([\d\.,\s]+)\]', response)
            if array_match:
                scores_str = array_match.group(1)
                scores = [float(x.strip()) for x in scores_str.split(',')]
                if len(scores) == expected_length:
                    scores = [max(0.1, min(3.0, score)) for score in scores]
                    return scores

            self.logger.warning(f"è¯„åˆ†è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: {response[:100]}")
            return [default_score] * expected_length

        except Exception as e:
            self.logger.error(f"è¯„åˆ†è§£æå¼‚å¸¸: {e}")
            return [default_score] * expected_length

    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMè·å–å“åº”"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                stream=True
            )

            content = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content += chunk.choices[0].delta.content

            return content
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise

    def _adjust_dimension_weights(self, round_idx):
        """æ ¹æ®è®­ç»ƒè¿›å±•åŠ¨æ€è°ƒæ•´ç»´åº¦æƒé‡ï¼ˆ5ä¸ªä¸“å®¶ï¼‰"""
        if round_idx < 5:
            # åˆæœŸæ›´é‡è§†æ•°æ®è´¨é‡å’Œæ¨¡å‹æ€§èƒ½
            self.dimension_weights = {
                'model_performance': 0.40,
                'data_quality': 0.30,
                'spatial_distribution': 0.15,
                'temporal_stability': 0.10,
                'traffic_pattern': 0.05
            }
        elif round_idx < 15:
            # ä¸­æœŸå¹³è¡¡å„ç»´åº¦ï¼ŒåŠ å¼ºç©ºé—´æ„ŸçŸ¥
            self.dimension_weights = {
                'model_performance': 0.35,
                'data_quality': 0.25,
                'spatial_distribution': 0.20,
                'temporal_stability': 0.15,
                'traffic_pattern': 0.05
            }
        else:
            # åæœŸæ›´é‡è§†é•¿æœŸç¨³å®šæ€§å’Œæµé‡æ¨¡å¼
            self.dimension_weights = {
                'model_performance': 0.30,
                'data_quality': 0.20,
                'spatial_distribution': 0.20,
                'temporal_stability': 0.20,
                'traffic_pattern': 0.10
            }

    def _get_stats_hash(self, client_stats):
        """ç”Ÿæˆå®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        key_data = []
        for stats in client_stats:
            key_data.append(f"{stats.client_id}_{stats.loss:.4f}")
        return hashlib.md5('_'.join(key_data).encode()).hexdigest()[:8]

    def get_dimension_analysis_summary(self, client_stats, round_idx):
        """è·å–ç»´åº¦åˆ†ææ‘˜è¦ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰"""
        dimension_scores = self._get_enhanced_dimension_scores(client_stats, round_idx)

        summary = {
            'round': round_idx,
            'dimension_weights': self.dimension_weights.copy(),
            'dimension_scores': {},
            'top_clients': {},
            'insights': []
        }

        # å„ç»´åº¦å¾—åˆ†ç»Ÿè®¡
        for dim, scores in dimension_scores.items():
            summary['dimension_scores'][dim] = {
                'mean': float(np.mean(scores)),
                'std': float(np.std(scores)),
                'min': float(np.min(scores)),
                'max': float(np.max(scores)),
                'scores': [float(s) for s in scores]
            }

            # æ‰¾å‡ºè¯¥ç»´åº¦æœ€é«˜åˆ†å®¢æˆ·ç«¯
            best_idx = np.argmax(scores)
            summary['top_clients'][dim] = {
                'client_id': str(client_stats[best_idx].client_id),
                'score': float(scores[best_idx])
            }

        # ç”Ÿæˆæ´å¯Ÿ
        final_scores = self._calculate_weighted_scores(dimension_scores)
        best_overall_idx = np.argmax(final_scores)
        worst_overall_idx = np.argmin(final_scores)

        summary['insights'] = [
            f"æœ€ä½³ç»¼åˆè¡¨ç°: å®¢æˆ·ç«¯ {client_stats[best_overall_idx].client_id} (å¾—åˆ†: {final_scores[best_overall_idx]:.3f})",
            f"éœ€è¦å…³æ³¨: å®¢æˆ·ç«¯ {client_stats[worst_overall_idx].client_id} (å¾—åˆ†: {final_scores[worst_overall_idx]:.3f})",
            f"æ•°æ®è´¨é‡æœ€é«˜: å®¢æˆ·ç«¯ {summary['top_clients']['data_quality']['client_id']}",
            f"ç©ºé—´åˆ†å¸ƒæœ€ä½³: å®¢æˆ·ç«¯ {summary['top_clients']['spatial_distribution']['client_id']}",
            f"æ¨¡å‹æ€§èƒ½æœ€ä½³: å®¢æˆ·ç«¯ {summary['top_clients']['model_performance']['client_id']}"
        ]

    def _print_expert_analysis(self, dimension: str, scores: List[float], client_stats: List):
        """æ‰“å°ä¸“å®¶åˆ†æç»“æœ"""
        print(f"   ğŸ“ˆ {dimension.replace('_', ' ').title()}ä¸“å®¶è¯„åˆ†ç»“æœ:")

        # æ˜¾ç¤ºæ¯ä¸ªå®¢æˆ·ç«¯çš„è¯„åˆ†
        for i, (score, stats) in enumerate(zip(scores, client_stats)):
            client_id = stats.client_id
            print(f"      åŸºç«™ {client_id}: {score:.3f}")

        # ç»Ÿè®¡åˆ†æ
        avg_score = np.mean(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        std_score = np.std(scores)

        print(
            f"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å¹³å‡={avg_score:.3f}, æœ€é«˜={max_score:.3f}, æœ€ä½={min_score:.3f}, æ ‡å‡†å·®={std_score:.3f}")

        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®å®¢æˆ·ç«¯
        best_idx = np.argmax(scores)
        worst_idx = np.argmin(scores)

        print(f"   ğŸ† æœ€ä½³: åŸºç«™ {client_stats[best_idx].client_id} ({scores[best_idx]:.3f})")
        print(f"   âš ï¸  å…³æ³¨: åŸºç«™ {client_stats[worst_idx].client_id} ({scores[worst_idx]:.3f})")

        # ç»´åº¦ç‰¹å®šçš„è¯¦ç»†åˆ†æ
        self._print_dimension_specific_insights(dimension, scores, client_stats)
        print()

    def _print_dimension_specific_insights(self, dimension: str, scores: List[float], client_stats: List):
        """æ‰“å°ç»´åº¦ç‰¹å®šçš„æ´å¯Ÿ"""
        if dimension == 'model_performance':
            # åˆ†ææŸå¤±åˆ†å¸ƒ
            losses = [stats.loss for stats in client_stats]
            print(f"   ğŸ’¡ æ€§èƒ½æ´å¯Ÿ: æŸå¤±èŒƒå›´ {min(losses):.4f}-{max(losses):.4f}")

        elif dimension == 'data_quality':
            # åˆ†ææ•°æ®è´¨é‡åˆ†å¸ƒ
            cvs = [stats.traffic_stats.get('coefficient_of_variation', 0) for stats in client_stats]
            stable_count = sum(1 for cv in cvs if cv < 0.5)
            print(f"   ğŸ’¡ è´¨é‡æ´å¯Ÿ: {stable_count}/{len(cvs)} ä¸ªåŸºç«™æ•°æ®ç¨³å®š (CV<0.5)")

        elif dimension == 'spatial_distribution':
            # åˆ†æåœ°ç†åˆ†å¸ƒ
            locations = [(stats.coordinates['lng'], stats.coordinates['lat']) for stats in client_stats]
            diversity = self._calculate_spatial_diversity(locations)
            print(f"   ğŸ’¡ ç©ºé—´æ´å¯Ÿ: åœ°ç†åˆ†æ•£åº¦ {diversity:.3f} (0-1, è¶Šé«˜è¶Šåˆ†æ•£)")

        elif dimension == 'temporal_stability':
            # åˆ†ææ—¶åºç¨³å®šæ€§
            trends = [stats.traffic_stats.get('trend', 'unknown') for stats in client_stats]
            stable_count = sum(1 for trend in trends if trend == 'stable')
            print(f"   ğŸ’¡ ç¨³å®šæ€§æ´å¯Ÿ: {stable_count}/{len(trends)} ä¸ªåŸºç«™è¶‹åŠ¿ç¨³å®š")

        elif dimension == 'traffic_pattern':
            # åˆ†ææµé‡æ¨¡å¼
            means = [stats.traffic_stats.get('mean', 0) for stats in client_stats]
            ideal_count = sum(1 for mean in means if 50 <= mean <= 500)
            print(f"   ğŸ’¡ æ¨¡å¼æ´å¯Ÿ: {ideal_count}/{len(means)} ä¸ªåŸºç«™æµé‡åœ¨ç†æƒ³èŒƒå›´ (50-500)")

    def _print_comprehensive_analysis(self, dimension_scores: Dict, client_stats: List, round_idx: int):
        """æ‰“å°ç»¼åˆåˆ†æç»“æœ"""
        print(f"ğŸ¯ ç»¼åˆåˆ†æä¸å†³ç­–")
        print(f"{'=' * 50}")

        # è®¡ç®—åŠ æƒæ€»åˆ†
        final_scores = self._calculate_weighted_scores(dimension_scores)
        aggregation_weights = self._scores_to_weights(final_scores)

        print(f"ğŸ“‹ å®¢æˆ·ç«¯ç»¼åˆè¯„ä¼°æ’å:")
        # åˆ›å»ºæ’å
        ranked_indices = np.argsort(final_scores)[::-1]  # ä»é«˜åˆ°ä½æ’åº

        for rank, idx in enumerate(ranked_indices, 1):
            client_id = client_stats[idx].client_id
            score = final_scores[idx]
            weight = aggregation_weights[idx]

            # æ‰¾å‡ºè¯¥å®¢æˆ·ç«¯çš„å¼ºé¡¹
            strengths = []
            for dim, scores in dimension_scores.items():
                if scores[idx] > np.mean(scores) + 0.1:  # é«˜äºå¹³å‡å€¼
                    strengths.append(dim.replace('_', ' '))

            strength_str = ', '.join(strengths[:2]) if strengths else 'å¹³è¡¡å‹'

            print(f"   {rank:2d}. åŸºç«™ {client_id}: ç»¼åˆåˆ† {score:.3f} â†’ èšåˆæƒé‡ {weight:.3%} | å¼ºé¡¹: {strength_str}")

        # å†³ç­–æ‘˜è¦
        print(f"\nğŸ” å†³ç­–æ‘˜è¦:")
        best_client_idx = ranked_indices[0]
        worst_client_idx = ranked_indices[-1]

        print(
            f"   â€¢ æœ€ä½³è¡¨ç°: åŸºç«™ {client_stats[best_client_idx].client_id} (æƒé‡: {aggregation_weights[best_client_idx]:.3%})")
        print(
            f"   â€¢ éœ€è¦å…³æ³¨: åŸºç«™ {client_stats[worst_client_idx].client_id} (æƒé‡: {aggregation_weights[worst_client_idx]:.3%})")

        # æƒé‡åˆ†å¸ƒåˆ†æ
        max_weight = max(aggregation_weights)
        min_weight = min(aggregation_weights)
        weight_ratio = max_weight / min_weight if min_weight > 0 else float('inf')

        print(f"   â€¢ æƒé‡é›†ä¸­åº¦: æœ€é«˜/æœ€ä½ = {weight_ratio:.1f}x")

        if weight_ratio > 5:
            print(f"   âš ï¸  æƒé‡åˆ†å¸ƒè¾ƒä¸ºé›†ä¸­ï¼Œå°‘æ•°å®¢æˆ·ç«¯ä¸»å¯¼èšåˆ")
        elif weight_ratio < 2:
            print(f"   âœ… æƒé‡åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œæ°‘ä¸»åŒ–èšåˆ")
        else:
            print(f"   ğŸ“Š æƒé‡åˆ†å¸ƒé€‚ä¸­ï¼Œå¹³è¡¡çš„èšåˆç­–ç•¥")

        # ä¸“å®¶ä¸€è‡´æ€§åˆ†æ
        self._analyze_expert_consensus(dimension_scores, client_stats)

        print(f"{'=' * 50}")

    def _analyze_expert_consensus(self, dimension_scores: Dict, client_stats: List):
        """åˆ†æä¸“å®¶ä¸€è‡´æ€§"""
        print(f"\nğŸ¤ ä¸“å®¶æ„è§ä¸€è‡´æ€§åˆ†æ:")

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯åœ¨å„ç»´åº¦çš„æ’å
        client_rankings = {}
        for i, stats in enumerate(client_stats):
            client_id = str(stats.client_id)
            rankings = []

            for dim, scores in dimension_scores.items():
                # è®¡ç®—è¯¥å®¢æˆ·ç«¯åœ¨æ­¤ç»´åº¦çš„æ’åï¼ˆ1ä¸ºæœ€å¥½ï¼‰
                sorted_indices = np.argsort(scores)[::-1]
                rank = np.where(sorted_indices == i)[0][0] + 1
                rankings.append(rank)

            client_rankings[client_id] = rankings

            # è®¡ç®—æ’åæ–¹å·®ï¼ˆä¸€è‡´æ€§æŒ‡æ ‡ï¼‰
            rank_variance = np.var(rankings)
            consensus_level = "é«˜" if rank_variance < 2 else "ä¸­" if rank_variance < 5 else "ä½"

            print(f"   åŸºç«™ {client_id}: æ’åæ–¹å·®={rank_variance:.1f} â†’ ä¸“å®¶ä¸€è‡´æ€§: {consensus_level}")

        # æ‰¾å‡ºä¸“å®¶æ„è§æœ€ä¸€è‡´å’Œæœ€åˆ†æ­§çš„å®¢æˆ·ç«¯
        variances = {}
        for client_id, rankings in client_rankings.items():
            variances[client_id] = np.var(rankings)

        most_consistent = min(variances.items(), key=lambda x: x[1])
        most_controversial = max(variances.items(), key=lambda x: x[1])

        print(f"   ğŸ¯ ä¸“å®¶æœ€è®¤åŒ: åŸºç«™ {most_consistent[0]} (æ–¹å·®: {most_consistent[1]:.1f})")
        print(f"   ğŸ¤” ä¸“å®¶æœ€åˆ†æ­§: åŸºç«™ {most_controversial[0]} (æ–¹å·®: {most_controversial[1]:.1f})")