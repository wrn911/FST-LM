# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¤šç»´åº¦LLMè¾…åŠ©è”é‚¦èšåˆå™¨ - åŸºäºçœŸå®æµé‡æ•°æ®çš„ä¸“å®¶è¯„åˆ†ç³»ç»Ÿ
"""

import json
import copy
import numpy as np
from typing import List, Dict, Optional
import torch
import logging
import hashlib
from .aggregation import FedAvgAggregator, LoRAFedAvgAggregator


class ExpertOutputConfig:
    """ä¸“å®¶è¾“å‡ºæ§åˆ¶é…ç½®"""

    def __init__(self,
                 show_expert_process: bool = True,
                 show_llm_response: bool = True,
                 show_detailed_analysis: bool = True,
                 show_aggregation_process: bool = True,
                 show_consensus_analysis: bool = True):
        self.show_expert_process = show_expert_process  # æ˜¾ç¤ºä¸“å®¶è¯„åˆ†è¿‡ç¨‹
        self.show_llm_response = show_llm_response  # æ˜¾ç¤ºLLMå®Œæ•´å“åº”
        self.show_detailed_analysis = show_detailed_analysis  # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
        self.show_aggregation_process = show_aggregation_process  # æ˜¾ç¤ºèšåˆè¿‡ç¨‹
        self.show_consensus_analysis = show_consensus_analysis  # æ˜¾ç¤ºä¸“å®¶ä¸€è‡´æ€§åˆ†æ

    @classmethod
    def minimal(cls):
        """æœ€å°è¾“å‡ºæ¨¡å¼ - åªæ˜¾ç¤ºå…³é”®ç»“æœ"""
        return cls(
            show_expert_process=True,
            show_llm_response=False,
            show_detailed_analysis=False,
            show_aggregation_process=False,
            show_consensus_analysis=False
        )

    @classmethod
    def detailed(cls):
        """è¯¦ç»†è¾“å‡ºæ¨¡å¼ - æ˜¾ç¤ºæ‰€æœ‰ä¿¡æ¯"""
        return cls(
            show_expert_process=True,
            show_llm_response=True,
            show_detailed_analysis=True,
            show_aggregation_process=True,
            show_consensus_analysis=True
        )


class EnhancedMultiDimensionalLLMAggregator:
    """å¢å¼ºç‰ˆå¤šç»´åº¦ä¸“å®¶è¯„åˆ†LLMèšåˆå™¨"""

    def __init__(self,
                 api_key: str = None,
                 model_name: str = "glm-4-flash-250414",
                 cache_rounds: int = 1,
                 min_confidence: float = 0.7,
                 is_lora_mode: bool = False,
                 dimensions: List[str] = None,
                 server_instance=None,
                 verbose: bool = True,
                 alpha_max: float = 0.9,
                 alpha_min: float = 0.2,
                 decay_type: str = 'sigmoid',
                 base_constraint: float = 0.25):
        """
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„ä¸“å®¶å†³ç­–è¿‡ç¨‹
        """
        self.api_key = api_key
        self.model_name = model_name
        self.cache_rounds = cache_rounds
        self.min_confidence = min_confidence
        self.is_lora_mode = is_lora_mode
        self.verbose = verbose  # æ§åˆ¶è¾“å‡ºè¯¦ç»†ç¨‹åº¦

        # å®ç”¨çš„ä¸“å®¶ç»´åº¦ï¼ˆåˆ é™¤ä¸šåŠ¡ä»·å€¼ä¸“å®¶ï¼‰
        self.dimensions = dimensions or [
            'model_performance',  # æ¨¡å‹æ€§èƒ½ä¸“å®¶
            'data_quality',  # æ•°æ®è´¨é‡ä¸“å®¶
            'spatial_distribution',  # ç©ºé—´åˆ†å¸ƒä¸“å®¶
            'temporal_stability',  # æ—¶åºç¨³å®šæ€§ä¸“å®¶
            'traffic_pattern'  # æµé‡æ¨¡å¼ä¸“å®¶ï¼ˆ5ä¸ªä¸“å®¶ï¼‰
        ]

        # åŠ¨æ€ç»´åº¦æƒé‡ï¼ˆæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´ï¼‰
        self.dimension_weights = self._initialize_dimension_weights()

        # ç¼“å­˜å’Œæ—¥å¿—
        self.score_cache = {}
        self.logger = logging.getLogger(__name__)
        self.server_instance = server_instance

        # è®¾ç½®å¤‡ç”¨èšåˆå™¨
        self.fallback_aggregator = LoRAFedAvgAggregator() if is_lora_mode else FedAvgAggregator()

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = None
        self._init_llm_client()

        # æ–°å¢ï¼šåŠ¨æ€èåˆå‚æ•°
        self.alpha_max = alpha_max
        self.alpha_min = alpha_min
        self.decay_type = decay_type
        self.base_constraint = base_constraint

        # æ–°å¢ï¼šå†å²æ•°æ®è·Ÿè¸ª
        self.client_contribution_history = {}  # {client_id: è´¡çŒ®åº¦å†å²}
        self.previous_weights = None  # ä¸Šä¸€è½®æƒé‡
        self.round_history = []  # è½®æ¬¡å†å²
        self.constraint_triggers = []  # çº¦æŸè§¦å‘è®°å½•

    def _initialize_dimension_weights(self):
        """åˆå§‹åŒ–ä¸“å®¶ç»´åº¦æƒé‡ï¼ˆ5ä¸ªä¸“å®¶ï¼‰"""
        return {
            'model_performance': 0.35,  # æ¨¡å‹è´¨é‡æœ€é‡è¦
            'data_quality': 0.25,  # æ•°æ®è´¨é‡æ¬¡ä¹‹
            'spatial_distribution': 0.15,  # åœ°ç†åˆ†å¸ƒ
            'temporal_stability': 0.15,  # æ—¶åºç¨³å®šæ€§
            'traffic_pattern': 0.10  # æµé‡æ¨¡å¼
        }

    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if self.api_key:
                from openai import OpenAI
                self.llm_client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://open.bigmodel.cn/api/paas/v4/"
                )
                self.logger.info("æˆåŠŸåˆå§‹åŒ–å¢å¼ºç‰ˆå¤šç»´åº¦LLMå®¢æˆ·ç«¯")
            else:
                self.logger.warning("æœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨èšåˆå™¨")
        except Exception as e:
            self.logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def aggregate(self, client_models: List[Dict], client_info: List[Dict] = None,
                  client_stats: List = None, round_idx: int = 0):
        """å¢å¼ºç‰ˆå¤šç»´åº¦è¯„åˆ†èšåˆ - æ·»åŠ åŠ¨æ€èåˆå’Œçº¦æŸæœºåˆ¶"""

        if not self.llm_client or not client_stats:
            print(f"\nâš ï¸  è½®æ¬¡ {round_idx}: æ— LLMå®¢æˆ·ç«¯æˆ–ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨èšåˆå™¨")
            return self.fallback_aggregator.aggregate(client_models)

        try:
            print(f"\nğŸš€ å¯åŠ¨å¢å¼ºç‰ˆåŠ¨æ€æƒé‡èšåˆ - è½®æ¬¡ {round_idx}")

            # === ç¬¬ä¸€æ­¥ï¼šLLMæ™ºèƒ½è¯„åˆ† ===
            dimension_scores = self._get_enhanced_dimension_scores(client_stats, round_idx)
            llm_weights = np.array(self._calculate_weighted_scores(dimension_scores))
            llm_weights = self._scores_to_weights(llm_weights)

            # === ç¬¬äºŒæ­¥ï¼šè®¡ç®—å†å²è´¡çŒ®åº¦å®‰å…¨æƒé‡ ===
            safe_weights = self.calculate_contribution_based_safe_weights(client_stats, round_idx)

            # === ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€æƒé‡èåˆ ===
            alpha = self.get_decay_factor(round_idx)
            fused_weights = alpha * np.array(llm_weights) + (1 - alpha) * safe_weights

            # === ç¬¬å››æ­¥ï¼šæƒé‡çº¦æŸä¿æŠ¤ ===
            final_weights, constraint_info = self.constrain_weights(
                fused_weights, self.previous_weights, round_idx
            )

            # === ç¬¬äº”æ­¥ï¼šæ‰§è¡Œèšåˆ ===
            result = self._weighted_aggregate(client_models, final_weights)

            # === ç¬¬å…­æ­¥ï¼šæ›´æ–°å†å²è®°å½• ===
            self._update_client_history(client_stats, final_weights, round_idx)
            self.previous_weights = final_weights
            self.constraint_triggers.append(constraint_info)

            # === ç¬¬ä¸ƒæ­¥ï¼šè¾“å‡ºå†³ç­–ä¿¡æ¯ ===
            self._log_enhanced_aggregation_info(round_idx, alpha, llm_weights,
                                                safe_weights, final_weights, constraint_info)

            return result

        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆèšåˆå¤±è´¥: {e}")
            return self.fallback_aggregator.aggregate(client_models)

    def _get_enhanced_dimension_scores(self, client_stats, round_idx):
        """è·å–å¢å¼ºç‰ˆå„ç»´åº¦è¯„åˆ†"""
        dimension_scores = {}

        if self.verbose:
            print(f"\n{'=' * 80}")
            print(f"ğŸ§  è½®æ¬¡ {round_idx} - ä¸“å®¶è¯„åˆ†ç³»ç»Ÿå†³ç­–è¿‡ç¨‹")
            print(f"{'=' * 80}")

            # æ˜¾ç¤ºå½“å‰ç»´åº¦æƒé‡
            print(f"ğŸ“Š å½“å‰ä¸“å®¶æƒé‡åˆ†é…:")
            for dim, weight in self.dimension_weights.items():
                print(f"   â€¢ {dim.replace('_', ' ').title()}: {weight:.1%}")
            print()

        for dimension in self.dimensions:
            try:
                if self.verbose:
                    print(f"ğŸ” {dimension.replace('_', ' ').title()}ä¸“å®¶æ­£åœ¨è¯„ä¼°...")

                # æ£€æŸ¥ç¼“å­˜
                cache_key = f"{dimension}_{round_idx}_{self._get_stats_hash(client_stats)}"
                if cache_key in self.score_cache:
                    dimension_scores[dimension] = self.score_cache[cache_key]
                    if self.verbose:
                        print(f"   âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ: {[f'{s:.2f}' for s in dimension_scores[dimension]]}")
                    continue

                # è°ƒç”¨å¯¹åº”çš„ä¸“å®¶è¯„åˆ†å‡½æ•°
                if self.verbose:
                    print(f"   ğŸ¤– è°ƒç”¨LLMè¿›è¡Œä¸“ä¸šè¯„åˆ†...")
                scores = getattr(self, f'_score_{dimension}')(client_stats, round_idx)
                dimension_scores[dimension] = scores

                # ç¼“å­˜ç»“æœ
                self.score_cache[cache_key] = scores

                # æ˜¾ç¤ºè¯„åˆ†ç»“æœå’Œåˆ†æ
                if self.verbose:
                    self._print_expert_analysis(dimension, scores, client_stats)

            except Exception as e:
                if self.verbose:
                    print(f"   âŒ {dimension}ç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                    print(f"   ğŸ”„ ä½¿ç”¨é»˜è®¤è¯„åˆ†: {[1.0] * len(client_stats)}")
                else:
                    self.logger.warning(f"{dimension}ç»´åº¦è¯„åˆ†å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤è¯„åˆ†ä½œä¸ºå¤‡ç”¨
                dimension_scores[dimension] = [1.0] * len(client_stats)

        # æ˜¾ç¤ºç»¼åˆåˆ†æ
        if self.verbose:
            self._print_comprehensive_analysis(dimension_scores, client_stats, round_idx)

        return dimension_scores

    def _score_model_performance(self, client_stats, round_idx):
        """æ¨¡å‹æ€§èƒ½ä¸“å®¶è¯„åˆ†"""
        performance_data = []
        for i, stats in enumerate(client_stats):
            # ç»“åˆå¤šä¸ªæ€§èƒ½æŒ‡æ ‡
            loss = stats.loss
            trend_info = getattr(stats, 'trend_info', {})

            performance_data.append({
                'client': i,
                'loss': f"{loss:.4f}",
                'trend': trend_info.get('description', 'unknown'),
                'improvement_rate': f"{trend_info.get('improvement_rate', 0):.1f}%",
                'participation': getattr(stats, 'participation_count', 1)
            })

        prompt = f"""ä½ æ˜¯æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸“å®¶ã€‚è¯·ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„æ¨¡å‹è®­ç»ƒæ€§èƒ½ç»™å‡ºè¯„åˆ†ã€‚

æ€§èƒ½æ•°æ®ï¼š
{json.dumps(performance_data, indent=2, ensure_ascii=False)}

è¯„åˆ†åŸåˆ™ï¼š
1. **æŸå¤±å€¼æƒé‡70%**: æŸå¤±è¶Šä½è¯„åˆ†è¶Šé«˜ï¼Œä½†è¦è­¦æƒ•è¿‡æ‹Ÿåˆ
2. **æ”¹è¿›è¶‹åŠ¿æƒé‡20%**: strongly_improving(1.5x), improving(1.2x), stable(1.0x), deteriorating(0.7x)
3. **å‚ä¸ç¨³å®šæ€§æƒé‡10%**: å‚ä¸è½®æ•°å¤šçš„å®¢æˆ·ç«¯æ›´å¯é 

ç‰¹æ®Šè€ƒè™‘ï¼š
- æŸå¤±å¼‚å¸¸ä½(<0.1)å¯èƒ½è¿‡æ‹Ÿåˆï¼Œé€‚å½“æƒ©ç½š
- æ”¹è¿›ç‡>20%çš„å®¢æˆ·ç«¯ç»™äºˆå¥–åŠ±
- æ–°å®¢æˆ·ç«¯(å‚ä¸<3è½®)é€‚å½“ä¿å®ˆè¯„åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.3-2.0ï¼Œç›´æ¥è¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_data_quality(self, client_stats, round_idx):
        """æ•°æ®è´¨é‡ä¸“å®¶è¯„åˆ†"""
        quality_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats
            quality_data.append({
                'client': i,
                'data_points': traffic.get('data_points', 0),
                'cv': f"{traffic.get('coefficient_of_variation', 0):.3f}",
                'iqr': f"{traffic.get('iqr', 0):.1f}",
                'trend_stability': traffic.get('trend', 'unknown'),
                'range_ratio': f"{traffic.get('max', 1) / max(traffic.get('min', 1), 0.1):.1f}"
            })

        prompt = f"""ä½ æ˜¯æ•°æ®è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºçœŸå®æµé‡ç»Ÿè®¡ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„æ•°æ®è´¨é‡è¯„åˆ†ã€‚

æ•°æ®è´¨é‡æŒ‡æ ‡ï¼š
{json.dumps(quality_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **æ•°æ®å……è¶³æ€§(30%)**: data_pointsè¶Šå¤šè¶Šå¥½ï¼Œ<100æƒ©ç½šï¼Œ>500å¥–åŠ±
2. **å˜å¼‚ç³»æ•°(40%)**: cv<0.5(ä¼˜ç§€1.5x), 0.5-1.0(è‰¯å¥½1.0x), >1.0(è¾ƒå·®0.7x)
3. **åˆ†å¸ƒåˆç†æ€§(20%)**: iqré€‚ä¸­æœ€å¥½ï¼Œè¿‡å¤§æˆ–è¿‡å°éƒ½ä¸å¥½
4. **è¶‹åŠ¿ç¨³å®šæ€§(10%)**: stable>increasing>decreasing

ç‰¹æ®Šå¤„ç†ï¼š
- cv>2.0è¡¨ç¤ºæä¸ç¨³å®šï¼Œä¸¥é‡æƒ©ç½š(0.4x)
- range_ratio>100è¡¨ç¤ºæ•°æ®å¼‚å¸¸ï¼Œæƒ©ç½š
- trend='stable'ä¸”cv<0.3ç»™äºˆå¥–åŠ±

è¯„åˆ†èŒƒå›´ï¼š0.2-1.8ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_spatial_distribution(self, client_stats, round_idx):
        """ç©ºé—´åˆ†å¸ƒä¸“å®¶è¯„åˆ†"""
        spatial_data = []
        locations = []

        for i, stats in enumerate(client_stats):
            lng, lat = stats.coordinates['lng'], stats.coordinates['lat']
            locations.append((lng, lat))
            spatial_data.append({
                'client': i,
                'lng': f"{lng:.3f}",
                'lat': f"{lat:.3f}",
                'location_type': self._classify_location_type(lng, lat)
            })

        # è®¡ç®—ç©ºé—´åˆ†æ•£åº¦
        diversity_score = self._calculate_spatial_diversity(locations)

        prompt = f"""ä½ æ˜¯ç©ºé—´åˆ†å¸ƒä¸“å®¶ã€‚è¯·ä¸º{len(client_stats)}ä¸ªåŸºç«™çš„åœ°ç†ä½ç½®ä»£è¡¨æ€§è¯„åˆ†ã€‚

ä½ç½®ä¿¡æ¯ï¼š
{json.dumps(spatial_data, indent=2, ensure_ascii=False)}

ç©ºé—´åˆ†æ•£åº¦è¯„åˆ†: {diversity_score:.3f} (0-1, è¶Šé«˜è¶Šåˆ†æ•£)

è¯„åˆ†åŸåˆ™ï¼š
1. **åˆ†æ•£æ€§å¥–åŠ±(50%)**: åŸºäºæ•´ä½“ç©ºé—´åˆ†æ•£åº¦ï¼Œè¶Šåˆ†æ•£è¶Šå¥½
2. **è¾¹ç¼˜ä»·å€¼(30%)**: ä½äºè¾¹ç¼˜çš„åŸºç«™æœ‰ç‹¬ç‰¹ä»·å€¼
3. **è¦†ç›–å‡è¡¡(20%)**: é¿å…æŸä¸ªåŒºåŸŸè¿‡åº¦é›†ä¸­

ç‰¹æ®Šè€ƒè™‘ï¼š
- åˆ†æ•£åº¦>0.7æ—¶æ‰€æœ‰åŸºç«™è·å¾—å¥–åŠ±
- ä½äºåœ°ç†è¾¹ç•Œçš„åŸºç«™é¢å¤–åŠ åˆ†
- è¿‡åº¦èšé›†çš„åŸºç«™ç¾¤ç»„å†…è¯„åˆ†é€’å‡

è¯„åˆ†èŒƒå›´ï¼š0.6-1.4ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_temporal_stability(self, client_stats, round_idx):
        """æ—¶åºç¨³å®šæ€§ä¸“å®¶è¯„åˆ†"""
        temporal_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats
            temporal_data.append({
                'client': i,
                'trend': traffic.get('trend', 'unknown'),
                'trend_slope': f"{traffic.get('trend_slope', 0):.4f}",
                'recent_vs_avg': f"{traffic.get('recent_mean', 0) / max(traffic.get('mean', 1), 0.1):.2f}",
                'cv': f"{traffic.get('coefficient_of_variation', 0):.3f}"
            })

        prompt = f"""ä½ æ˜¯æ—¶åºç¨³å®šæ€§ä¸“å®¶ã€‚è¯·è¯„ä¼°{len(client_stats)}ä¸ªåŸºç«™çš„æ—¶é—´åºåˆ—ç¨³å®šæ€§ã€‚

æ—¶åºç‰¹å¾ï¼š
{json.dumps(temporal_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **è¶‹åŠ¿ç¨³å®šæ€§(40%)**: stable(1.2x), increasing(1.0x), decreasing(0.8x)
2. **æ–œç‡åˆç†æ€§(30%)**: |slope|<0.01(å¥½), 0.01-0.05(ä¸€èˆ¬), >0.05(å·®)
3. **æœ€è¿‘æœŸä¸€è‡´æ€§(20%)**: recent_vs_avgåœ¨0.8-1.2ä¸ºå¥½
4. **æ³¢åŠ¨æ§åˆ¶(10%)**: cv<0.5ä¸ºç¨³å®š

æ—¶åºè´¨é‡è¯„çº§ï¼š
- ä¼˜ç§€: stable + low_cv + ä¸€è‡´æ€§å¥½ â†’ 1.3-1.6åˆ†
- è‰¯å¥½: è½»å¾®æ³¢åŠ¨ä½†æ•´ä½“ç¨³å®š â†’ 1.0-1.3åˆ†  
- ä¸€èˆ¬: æœ‰æ˜æ˜¾è¶‹åŠ¿ä½†å¯æ§ â†’ 0.7-1.0åˆ†
- è¾ƒå·®: é«˜æ³¢åŠ¨æˆ–å¼‚å¸¸è¶‹åŠ¿ â†’ 0.4-0.7åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.4-1.6ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    def _score_traffic_pattern(self, client_stats, round_idx):
        """æµé‡æ¨¡å¼ä¸“å®¶è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆ - åŸºäºçœŸå®ç»Ÿè®¡ç‰¹å¾ï¼‰"""
        pattern_data = []
        for i, stats in enumerate(client_stats):
            traffic = stats.traffic_stats

            # å¢å¼ºçš„æµé‡æ¨¡å¼åˆ†æ
            mean_traffic = traffic.get('mean', 0)
            cv = traffic.get('coefficient_of_variation', 0)
            trend = traffic.get('trend', 'unknown')
            trend_slope = abs(traffic.get('trend_slope', 0))
            iqr = traffic.get('iqr', 0)

            pattern_data.append({
                'client': i,
                'avg_traffic': f"{mean_traffic:.1f}",
                'traffic_level': self._classify_traffic_level(mean_traffic),
                'stability': 'stable' if cv < 0.5 else 'moderate' if cv < 1.0 else 'unstable',
                'trend_strength': 'weak' if trend_slope < 0.01 else 'moderate' if trend_slope < 0.05 else 'strong',
                'trend_direction': trend,
                'variability': f"{cv:.3f}",
                'data_spread': f"{iqr:.1f}",
                'pattern_quality': self._assess_pattern_quality(traffic)
            })

        prompt = f"""ä½ æ˜¯æµé‡æ¨¡å¼ä¸“å®¶ã€‚è¯·åŸºäºçœŸå®æµé‡ç»Ÿè®¡è¯„ä¼°{len(client_stats)}ä¸ªåŸºç«™çš„æµé‡æ¨¡å¼è´¨é‡ã€‚

è¯¦ç»†æµé‡æ¨¡å¼åˆ†æï¼š
{json.dumps(pattern_data, indent=2, ensure_ascii=False)}

è¯„åˆ†æ ‡å‡†ï¼š
1. **æµé‡ç¨³å®šæ€§(40%)**: 
   - stable(cv<0.5): 1.3x - æ•°æ®å¯é ï¼Œé€‚åˆè®­ç»ƒ
   - moderate(0.5â‰¤cv<1.0): 1.0x - ä¸­ç­‰è´¨é‡
   - unstable(cvâ‰¥1.0): 0.7x - ä¸ç¨³å®šï¼Œå½±å“å­¦ä¹ 

2. **æµé‡æ°´å¹³åˆç†æ€§(25%)**: 
   - ä¸­ç­‰æµé‡(50-500): 1.2x - æœ€ä½³è®­ç»ƒåŒºé—´
   - é«˜æµé‡(>500): 1.1x - é‡è¦ä½†å¯èƒ½æœ‰å™ªå£°
   - ä½æµé‡(<50): 0.9x - ä¿¡å·è¾ƒå¼±

3. **è¶‹åŠ¿ç‰¹å¾(20%)**: 
   - weak trend + stable: 1.2x - ç†æƒ³çš„å¹³ç¨³æ¨¡å¼
   - moderate trend: 1.0x - æ­£å¸¸å˜åŒ–
   - strong trend: 0.8x - å¯èƒ½å­˜åœ¨å¼‚å¸¸

4. **æ•°æ®åˆ†å¸ƒè´¨é‡(15%)**: 
   - é€‚ä¸­çš„IQR(10-100): 1.1x
   - è¿‡å°æˆ–è¿‡å¤§çš„IQR: 0.9x

ç‰¹æ®ŠåŠ åˆ†é¡¹ï¼š
- stable + moderate traffic + weak trend: +0.2åˆ†
- æ•°æ®è´¨é‡è¯„çº§ä¸º'good'çš„åŸºç«™: +0.1åˆ†

è¯„åˆ†èŒƒå›´ï¼š0.4-1.8ï¼Œè¾“å‡ºæ•°ç»„ï¼š[åˆ†æ•°1, åˆ†æ•°2, ...]

è¯„åˆ†ï¼š"""

        response = self._call_llm(prompt)
        return self._parse_scores(response, len(client_stats), default_score=1.0)

    # è¾…åŠ©å‡½æ•°
    def _classify_location_type(self, lng, lat):
        """åˆ†ç±»ä½ç½®ç±»å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…åœ°ç†ä¿¡æ¯è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ†ç±»
        if abs(lng - 116.3) < 0.1 and abs(lat - 39.9) < 0.1:
            return "core_business"
        elif abs(lng - 116.3) < 0.5 and abs(lat - 39.9) < 0.5:
            return "urban_area"
        else:
            return "suburban"

    def _calculate_spatial_diversity(self, locations):
        """è®¡ç®—ç©ºé—´åˆ†æ•£åº¦"""
        if len(locations) < 2:
            return 0.5

        distances = []
        for i in range(len(locations)):
            for j in range(i + 1, len(locations)):
                lng1, lat1 = locations[i]
                lng2, lat2 = locations[j]
                dist = ((lng1 - lng2) ** 2 + (lat1 - lat2) ** 2) ** 0.5
                distances.append(dist)

        avg_distance = np.mean(distances)
        max_distance = max(distances)

        # å½’ä¸€åŒ–åˆ°0-1
        diversity = min(1.0, avg_distance / (max_distance + 1e-6))
        return diversity

    def _classify_traffic_level(self, mean_traffic):
        """åˆ†ç±»æµé‡æ°´å¹³"""
        if mean_traffic > 300:
            return "high"
        elif mean_traffic > 100:
            return "medium"
        else:
            return "low"

    def _classify_distribution_type(self, traffic_stats):
        """åˆ†ç±»æµé‡åˆ†å¸ƒç±»å‹"""
        cv = traffic_stats.get('coefficient_of_variation', 0)
        if cv < 0.3:
            return "stable"
        elif cv < 0.8:
            return "moderate_variation"
        else:
            return "high_variation"

    def _assess_pattern_quality(self, traffic_stats):
        """è¯„ä¼°æµé‡æ¨¡å¼è´¨é‡ï¼ˆæ–°å¢ï¼‰"""
        cv = traffic_stats.get('coefficient_of_variation', 0)
        mean = traffic_stats.get('mean', 0)
        trend = traffic_stats.get('trend', 'unknown')

        # ç»¼åˆè¯„ä¼°æ¨¡å¼è´¨é‡
        quality_score = 0

        # ç¨³å®šæ€§è¯„åˆ†
        if cv < 0.3:
            quality_score += 3  # éå¸¸ç¨³å®š
        elif cv < 0.7:
            quality_score += 2  # è¾ƒç¨³å®š
        else:
            quality_score += 1  # ä¸ç¨³å®š

        # æµé‡æ°´å¹³è¯„åˆ†
        if 50 <= mean <= 500:
            quality_score += 2  # åˆç†èŒƒå›´
        else:
            quality_score += 1  # åç¦»ç†æƒ³èŒƒå›´

        # è¶‹åŠ¿è¯„åˆ†
        if trend == 'stable':
            quality_score += 2
        elif trend in ['increasing', 'decreasing']:
            quality_score += 1

        # è½¬æ¢ä¸ºè´¨é‡ç­‰çº§
        if quality_score >= 6:
            return 'excellent'
        elif quality_score >= 5:
            return 'good'
        elif quality_score >= 3:
            return 'fair'
        else:
            return 'poor'

    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ–¹æ³•...
    def _calculate_weighted_scores(self, dimension_scores):
        """è®¡ç®—åŠ æƒæ€»åˆ†"""
        num_clients = len(next(iter(dimension_scores.values())))
        final_scores = [0.0] * num_clients

        total_weight = 0.0
        for dimension, scores in dimension_scores.items():
            weight = self.dimension_weights.get(dimension, 1.0 / len(self.dimensions))
            total_weight += weight

            for i in range(num_clients):
                final_scores[i] += scores[i] * weight

        # å½’ä¸€åŒ–
        if total_weight != 1.0:
            final_scores = [score / total_weight for score in final_scores]

        self.logger.info(f"åŠ æƒæ€»åˆ†: {[f'{s:.3f}' for s in final_scores]}")
        return final_scores

    def _scores_to_weights(self, scores):
        """å°†è¯„åˆ†è½¬æ¢ä¸ºèšåˆæƒé‡"""
        scores_array = np.array(scores)
        temperature = 1.2  # è°ƒæ•´æ¸©åº¦å‚æ•°
        scores_array = scores_array / temperature
        exp_scores = np.exp(scores_array - np.max(scores_array))
        weights = exp_scores / np.sum(exp_scores)

        self.logger.info(f"Softmaxæƒé‡: {[f'{w:.3f}' for w in weights]}")
        return weights.tolist()

    def _weighted_aggregate(self, client_models: List[Dict], weights: List[float]) -> Dict:
        """æ‰§è¡ŒåŠ æƒèšåˆå¹¶æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹"""
        if not client_models:
            raise ValueError("å®¢æˆ·ç«¯æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        print(f"\nâš™ï¸  æ‰§è¡ŒåŠ æƒæ¨¡å‹èšåˆ:")
        print(f"   å‚ä¸èšåˆçš„å®¢æˆ·ç«¯æ•°é‡: {len(client_models)}")
        print(f"   èšåˆæƒé‡: {[f'{w:.3%}' for w in weights]}")

        # æ˜¾ç¤ºæƒé‡åˆ†å¸ƒç»Ÿè®¡
        max_weight = max(weights)
        min_weight = min(weights)
        avg_weight = sum(weights) / len(weights)

        print(f"   æƒé‡ç»Ÿè®¡: æœ€å¤§={max_weight:.3%}, æœ€å°={min_weight:.3%}, å¹³å‡={avg_weight:.3%}")

        aggregated_model = {}
        total_params = 0

        # ç»Ÿè®¡å‚æ•°ä¿¡æ¯
        param_info = {}
        for key in client_models[0].keys():
            param_shape = client_models[0][key].shape
            param_count = client_models[0][key].numel()
            param_info[key] = {'shape': param_shape, 'count': param_count}
            total_params += param_count

        print(f"   æ¨¡å‹å‚æ•°ç»Ÿè®¡: æ€»è®¡ {total_params:,} ä¸ªå‚æ•°")

        if self.is_lora_mode:
            lora_params = sum(
                1 for key in param_info.keys() if any(lora_key in key for lora_key in ['lora_A', 'lora_B']))
            print(f"   LoRAæ¨¡å¼: {lora_params} ä¸ªLoRAå‚æ•°æ¨¡å—")

        # æ‰§è¡Œèšåˆ
        print(f"   ğŸ”„ æ­£åœ¨èšåˆæ¨¡å‹å‚æ•°...")

        for key in client_models[0].keys():
            aggregated_model[key] = torch.zeros_like(client_models[0][key])
            for i, client_model in enumerate(client_models):
                if key in client_model:
                    aggregated_model[key] += client_model[key] * weights[i]

        print(f"   âœ… å‚æ•°èšåˆå®Œæˆ")

        # æ˜¾ç¤ºèšåˆæ•ˆæœåˆ†æ
        print(f"   ğŸ“Š èšåˆæ•ˆæœåˆ†æ:")
        print(f"      â€¢ å‚æ•°æ›´æ–°å®Œæˆ: {len(aggregated_model)} ä¸ªæ¨¡å—")

        if self.is_lora_mode:
            print(f"      â€¢ LoRAå‚æ•°æ€»é‡: {total_params:,}")
            print(f"      â€¢ é€šä¿¡æ•ˆç‡: 99%+ (ä»…ä¼ è¾“LoRAå‚æ•°)")

        # æƒé‡åˆ†å¸ƒåˆ†æ
        weight_entropy = -sum(w * np.log(w + 1e-10) for w in weights)  # ä¿¡æ¯ç†µ
        max_entropy = np.log(len(weights))  # æœ€å¤§ç†µï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        diversity_ratio = weight_entropy / max_entropy

        print(f"      â€¢ æƒé‡å¤šæ ·æ€§: {diversity_ratio:.2%} (100%ä¸ºå®Œå…¨å‡åŒ€)")

        if diversity_ratio > 0.9:
            print(f"      â€¢ å†³ç­–ç±»å‹: æ°‘ä¸»åŒ–èšåˆ (æƒé‡åˆ†å¸ƒå‡åŒ€)")
        elif diversity_ratio > 0.7:
            print(f"      â€¢ å†³ç­–ç±»å‹: å¹³è¡¡èšåˆ (æƒé‡é€‚åº¦é›†ä¸­)")
        else:
            print(f"      â€¢ å†³ç­–ç±»å‹: ç²¾è‹±èšåˆ (æƒé‡é«˜åº¦é›†ä¸­)")

        return aggregated_model

    def _parse_scores(self, response, expected_length, default_score=1.0):
        """è§£æLLMè¿”å›çš„è¯„åˆ†"""
        try:
            import re
            array_match = re.search(r'\[([\d\.,\s]+)\]', response)
            if array_match:
                scores_str = array_match.group(1)
                scores = [float(x.strip()) for x in scores_str.split(',')]
                if len(scores) == expected_length:
                    scores = [max(0.1, min(3.0, score)) for score in scores]
                    return scores

            self.logger.warning(f"è¯„åˆ†è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: {response[:100]}")
            return [default_score] * expected_length

        except Exception as e:
            self.logger.error(f"è¯„åˆ†è§£æå¼‚å¸¸: {e}")
            return [default_score] * expected_length

    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMè·å–å“åº”"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                stream=True
            )

            content = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content += chunk.choices[0].delta.content

            return content
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise

    def _adjust_dimension_weights(self, round_idx):
        """æ ¹æ®è®­ç»ƒè¿›å±•åŠ¨æ€è°ƒæ•´ç»´åº¦æƒé‡ï¼ˆ5ä¸ªä¸“å®¶ï¼‰"""
        if round_idx < 5:
            # åˆæœŸæ›´é‡è§†æ•°æ®è´¨é‡å’Œæ¨¡å‹æ€§èƒ½
            self.dimension_weights = {
                'model_performance': 0.40,
                'data_quality': 0.30,
                'spatial_distribution': 0.15,
                'temporal_stability': 0.10,
                'traffic_pattern': 0.05
            }
        elif round_idx < 15:
            # ä¸­æœŸå¹³è¡¡å„ç»´åº¦ï¼ŒåŠ å¼ºç©ºé—´æ„ŸçŸ¥
            self.dimension_weights = {
                'model_performance': 0.35,
                'data_quality': 0.25,
                'spatial_distribution': 0.20,
                'temporal_stability': 0.15,
                'traffic_pattern': 0.05
            }
        else:
            # åæœŸæ›´é‡è§†é•¿æœŸç¨³å®šæ€§å’Œæµé‡æ¨¡å¼
            self.dimension_weights = {
                'model_performance': 0.30,
                'data_quality': 0.20,
                'spatial_distribution': 0.20,
                'temporal_stability': 0.20,
                'traffic_pattern': 0.10
            }

    def _get_stats_hash(self, client_stats):
        """ç”Ÿæˆå®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        key_data = []
        for stats in client_stats:
            key_data.append(f"{stats.client_id}_{stats.loss:.4f}")
        return hashlib.md5('_'.join(key_data).encode()).hexdigest()[:8]

    def get_dimension_analysis_summary(self, client_stats, round_idx):
        """è·å–ç»´åº¦åˆ†ææ‘˜è¦ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰"""
        dimension_scores = self._get_enhanced_dimension_scores(client_stats, round_idx)

        summary = {
            'round': round_idx,
            'dimension_weights': self.dimension_weights.copy(),
            'dimension_scores': {},
            'top_clients': {},
            'insights': []
        }

        # å„ç»´åº¦å¾—åˆ†ç»Ÿè®¡
        for dim, scores in dimension_scores.items():
            summary['dimension_scores'][dim] = {
                'mean': float(np.mean(scores)),
                'std': float(np.std(scores)),
                'min': float(np.min(scores)),
                'max': float(np.max(scores)),
                'scores': [float(s) for s in scores]
            }

            # æ‰¾å‡ºè¯¥ç»´åº¦æœ€é«˜åˆ†å®¢æˆ·ç«¯
            best_idx = np.argmax(scores)
            summary['top_clients'][dim] = {
                'client_id': str(client_stats[best_idx].client_id),
                'score': float(scores[best_idx])
            }

        # ç”Ÿæˆæ´å¯Ÿ
        final_scores = self._calculate_weighted_scores(dimension_scores)
        best_overall_idx = np.argmax(final_scores)
        worst_overall_idx = np.argmin(final_scores)

        summary['insights'] = [
            f"æœ€ä½³ç»¼åˆè¡¨ç°: å®¢æˆ·ç«¯ {client_stats[best_overall_idx].client_id} (å¾—åˆ†: {final_scores[best_overall_idx]:.3f})",
            f"éœ€è¦å…³æ³¨: å®¢æˆ·ç«¯ {client_stats[worst_overall_idx].client_id} (å¾—åˆ†: {final_scores[worst_overall_idx]:.3f})",
            f"æ•°æ®è´¨é‡æœ€é«˜: å®¢æˆ·ç«¯ {summary['top_clients']['data_quality']['client_id']}",
            f"ç©ºé—´åˆ†å¸ƒæœ€ä½³: å®¢æˆ·ç«¯ {summary['top_clients']['spatial_distribution']['client_id']}",
            f"æ¨¡å‹æ€§èƒ½æœ€ä½³: å®¢æˆ·ç«¯ {summary['top_clients']['model_performance']['client_id']}"
        ]

    def _print_expert_analysis(self, dimension: str, scores: List[float], client_stats: List):
        """æ‰“å°ä¸“å®¶åˆ†æç»“æœ"""
        print(f"   ğŸ“ˆ {dimension.replace('_', ' ').title()}ä¸“å®¶è¯„åˆ†ç»“æœ:")

        # æ˜¾ç¤ºæ¯ä¸ªå®¢æˆ·ç«¯çš„è¯„åˆ†
        for i, (score, stats) in enumerate(zip(scores, client_stats)):
            client_id = stats.client_id
            print(f"      åŸºç«™ {client_id}: {score:.3f}")

        # ç»Ÿè®¡åˆ†æ
        avg_score = np.mean(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        std_score = np.std(scores)

        print(
            f"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å¹³å‡={avg_score:.3f}, æœ€é«˜={max_score:.3f}, æœ€ä½={min_score:.3f}, æ ‡å‡†å·®={std_score:.3f}")

        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®å®¢æˆ·ç«¯
        best_idx = np.argmax(scores)
        worst_idx = np.argmin(scores)

        print(f"   ğŸ† æœ€ä½³: åŸºç«™ {client_stats[best_idx].client_id} ({scores[best_idx]:.3f})")
        print(f"   âš ï¸  å…³æ³¨: åŸºç«™ {client_stats[worst_idx].client_id} ({scores[worst_idx]:.3f})")

        # ç»´åº¦ç‰¹å®šçš„è¯¦ç»†åˆ†æ
        self._print_dimension_specific_insights(dimension, scores, client_stats)
        print()

    def _print_dimension_specific_insights(self, dimension: str, scores: List[float], client_stats: List):
        """æ‰“å°ç»´åº¦ç‰¹å®šçš„æ´å¯Ÿ"""
        if dimension == 'model_performance':
            # åˆ†ææŸå¤±åˆ†å¸ƒ
            losses = [stats.loss for stats in client_stats]
            print(f"   ğŸ’¡ æ€§èƒ½æ´å¯Ÿ: æŸå¤±èŒƒå›´ {min(losses):.4f}-{max(losses):.4f}")

        elif dimension == 'data_quality':
            # åˆ†ææ•°æ®è´¨é‡åˆ†å¸ƒ
            cvs = [stats.traffic_stats.get('coefficient_of_variation', 0) for stats in client_stats]
            stable_count = sum(1 for cv in cvs if cv < 0.5)
            print(f"   ğŸ’¡ è´¨é‡æ´å¯Ÿ: {stable_count}/{len(cvs)} ä¸ªåŸºç«™æ•°æ®ç¨³å®š (CV<0.5)")

        elif dimension == 'spatial_distribution':
            # åˆ†æåœ°ç†åˆ†å¸ƒ
            locations = [(stats.coordinates['lng'], stats.coordinates['lat']) for stats in client_stats]
            diversity = self._calculate_spatial_diversity(locations)
            print(f"   ğŸ’¡ ç©ºé—´æ´å¯Ÿ: åœ°ç†åˆ†æ•£åº¦ {diversity:.3f} (0-1, è¶Šé«˜è¶Šåˆ†æ•£)")

        elif dimension == 'temporal_stability':
            # åˆ†ææ—¶åºç¨³å®šæ€§
            trends = [stats.traffic_stats.get('trend', 'unknown') for stats in client_stats]
            stable_count = sum(1 for trend in trends if trend == 'stable')
            print(f"   ğŸ’¡ ç¨³å®šæ€§æ´å¯Ÿ: {stable_count}/{len(trends)} ä¸ªåŸºç«™è¶‹åŠ¿ç¨³å®š")

        elif dimension == 'traffic_pattern':
            # åˆ†ææµé‡æ¨¡å¼
            means = [stats.traffic_stats.get('mean', 0) for stats in client_stats]
            ideal_count = sum(1 for mean in means if 50 <= mean <= 500)
            print(f"   ğŸ’¡ æ¨¡å¼æ´å¯Ÿ: {ideal_count}/{len(means)} ä¸ªåŸºç«™æµé‡åœ¨ç†æƒ³èŒƒå›´ (50-500)")

    def _print_comprehensive_analysis(self, dimension_scores: Dict, client_stats: List, round_idx: int):
        """æ‰“å°ç»¼åˆåˆ†æç»“æœ"""
        print(f"ğŸ¯ ç»¼åˆåˆ†æä¸å†³ç­–")
        print(f"{'=' * 50}")

        # è®¡ç®—åŠ æƒæ€»åˆ†
        final_scores = self._calculate_weighted_scores(dimension_scores)
        aggregation_weights = self._scores_to_weights(final_scores)

        print(f"ğŸ“‹ å®¢æˆ·ç«¯ç»¼åˆè¯„ä¼°æ’å:")
        # åˆ›å»ºæ’å
        ranked_indices = np.argsort(final_scores)[::-1]  # ä»é«˜åˆ°ä½æ’åº

        for rank, idx in enumerate(ranked_indices, 1):
            client_id = client_stats[idx].client_id
            score = final_scores[idx]
            weight = aggregation_weights[idx]

            # æ‰¾å‡ºè¯¥å®¢æˆ·ç«¯çš„å¼ºé¡¹
            strengths = []
            for dim, scores in dimension_scores.items():
                if scores[idx] > np.mean(scores) + 0.1:  # é«˜äºå¹³å‡å€¼
                    strengths.append(dim.replace('_', ' '))

            strength_str = ', '.join(strengths[:2]) if strengths else 'å¹³è¡¡å‹'

            print(f"   {rank:2d}. åŸºç«™ {client_id}: ç»¼åˆåˆ† {score:.3f} â†’ èšåˆæƒé‡ {weight:.3%} | å¼ºé¡¹: {strength_str}")

        # å†³ç­–æ‘˜è¦
        print(f"\nğŸ” å†³ç­–æ‘˜è¦:")
        best_client_idx = ranked_indices[0]
        worst_client_idx = ranked_indices[-1]

        print(
            f"   â€¢ æœ€ä½³è¡¨ç°: åŸºç«™ {client_stats[best_client_idx].client_id} (æƒé‡: {aggregation_weights[best_client_idx]:.3%})")
        print(
            f"   â€¢ éœ€è¦å…³æ³¨: åŸºç«™ {client_stats[worst_client_idx].client_id} (æƒé‡: {aggregation_weights[worst_client_idx]:.3%})")

        # æƒé‡åˆ†å¸ƒåˆ†æ
        max_weight = max(aggregation_weights)
        min_weight = min(aggregation_weights)
        weight_ratio = max_weight / min_weight if min_weight > 0 else float('inf')

        print(f"   â€¢ æƒé‡é›†ä¸­åº¦: æœ€é«˜/æœ€ä½ = {weight_ratio:.1f}x")

        if weight_ratio > 5:
            print(f"   âš ï¸  æƒé‡åˆ†å¸ƒè¾ƒä¸ºé›†ä¸­ï¼Œå°‘æ•°å®¢æˆ·ç«¯ä¸»å¯¼èšåˆ")
        elif weight_ratio < 2:
            print(f"   âœ… æƒé‡åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œæ°‘ä¸»åŒ–èšåˆ")
        else:
            print(f"   ğŸ“Š æƒé‡åˆ†å¸ƒé€‚ä¸­ï¼Œå¹³è¡¡çš„èšåˆç­–ç•¥")

        # ä¸“å®¶ä¸€è‡´æ€§åˆ†æ
        self._analyze_expert_consensus(dimension_scores, client_stats)

        print(f"{'=' * 50}")

    def _analyze_expert_consensus(self, dimension_scores: Dict, client_stats: List):
        """åˆ†æä¸“å®¶ä¸€è‡´æ€§"""
        print(f"\nğŸ¤ ä¸“å®¶æ„è§ä¸€è‡´æ€§åˆ†æ:")

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯åœ¨å„ç»´åº¦çš„æ’å
        client_rankings = {}
        for i, stats in enumerate(client_stats):
            client_id = str(stats.client_id)
            rankings = []

            for dim, scores in dimension_scores.items():
                # è®¡ç®—è¯¥å®¢æˆ·ç«¯åœ¨æ­¤ç»´åº¦çš„æ’åï¼ˆ1ä¸ºæœ€å¥½ï¼‰
                sorted_indices = np.argsort(scores)[::-1]
                rank = np.where(sorted_indices == i)[0][0] + 1
                rankings.append(rank)

            client_rankings[client_id] = rankings

            # è®¡ç®—æ’åæ–¹å·®ï¼ˆä¸€è‡´æ€§æŒ‡æ ‡ï¼‰
            rank_variance = np.var(rankings)
            consensus_level = "é«˜" if rank_variance < 2 else "ä¸­" if rank_variance < 5 else "ä½"

            print(f"   åŸºç«™ {client_id}: æ’åæ–¹å·®={rank_variance:.1f} â†’ ä¸“å®¶ä¸€è‡´æ€§: {consensus_level}")

        # æ‰¾å‡ºä¸“å®¶æ„è§æœ€ä¸€è‡´å’Œæœ€åˆ†æ­§çš„å®¢æˆ·ç«¯
        variances = {}
        for client_id, rankings in client_rankings.items():
            variances[client_id] = np.var(rankings)

        most_consistent = min(variances.items(), key=lambda x: x[1])
        most_controversial = max(variances.items(), key=lambda x: x[1])

        print(f"   ğŸ¯ ä¸“å®¶æœ€è®¤åŒ: åŸºç«™ {most_consistent[0]} (æ–¹å·®: {most_consistent[1]:.1f})")
        print(f"   ğŸ¤” ä¸“å®¶æœ€åˆ†æ­§: åŸºç«™ {most_controversial[0]} (æ–¹å·®: {most_controversial[1]:.1f})")

    def get_decay_factor(self, round_idx, total_rounds=100):
        """è®¡ç®—åŠ¨æ€è¡°å‡å› å­Î±(t)"""
        t = round_idx / total_rounds

        if self.decay_type == 'sigmoid':
            # Så‹è¡°å‡
            k = 6.0
            t_mid = 0.5
            alpha = self.alpha_min + (self.alpha_max - self.alpha_min) / \
                    (1 + np.exp(k * (t - t_mid)))
        elif self.decay_type == 'exponential':
            # æŒ‡æ•°è¡°å‡
            beta = 2.0
            alpha = self.alpha_min + (self.alpha_max - self.alpha_min) * \
                    np.exp(-beta * t)
        else:  # linear
            # çº¿æ€§è¡°å‡
            alpha = self.alpha_max - (self.alpha_max - self.alpha_min) * t

        return np.clip(alpha, self.alpha_min, self.alpha_max)

    def calculate_contribution_based_safe_weights(self, client_stats, round_idx):
        """è®¡ç®—åŸºäºå†å²è´¡çŒ®åº¦çš„å®‰å…¨æƒé‡"""

        if round_idx < 3:
            # å‰3è½®ä½¿ç”¨æ ·æœ¬é‡æƒé‡
            return self._get_sample_weighted_safe_weights(client_stats)

        contribution_scores = []

        for stats in client_stats:
            client_id = str(stats.client_id)

            # è®¡ç®—ä¸‰ä¸ªç»´åº¦è¯„åˆ†
            stability = self._calculate_participation_stability(client_id, round_idx)
            quality = self._calculate_gradient_quality(client_id)
            consistency = self._calculate_cooperation_consistency(client_id)

            # åŠ æƒç»„åˆ
            contribution_score = (
                    0.4 * stability +
                    0.35 * quality +
                    0.25 * consistency
            )

            contribution_scores.append(contribution_score)

        # å½’ä¸€åŒ–ä¸ºæƒé‡
        contribution_scores = np.array(contribution_scores)
        min_weight = 0.05  # æœ€å°æƒé‡ä¿éšœ
        adjusted_scores = contribution_scores + min_weight
        safe_weights = adjusted_scores / np.sum(adjusted_scores)

        return safe_weights

    def _get_sample_weighted_safe_weights(self, client_stats):
        """åŸºäºæ ·æœ¬é‡çš„å®‰å…¨æƒé‡ï¼ˆå›é€€ç­–ç•¥ï¼‰"""
        if hasattr(client_stats[0], 'num_samples'):
            sample_counts = np.array([stats.num_samples for stats in client_stats])
        else:
            # å¦‚æœæ²¡æœ‰æ ·æœ¬æ•°ä¿¡æ¯ï¼Œä½¿ç”¨å‡åŒ€æƒé‡
            return np.ones(len(client_stats)) / len(client_stats)

        weights = sample_counts / np.sum(sample_counts)
        return weights

    def _calculate_participation_stability(self, client_id, round_idx):
        """è®¡ç®—å‚ä¸ç¨³å®šæ€§"""
        if client_id not in self.client_contribution_history:
            return 0.3  # æ–°å®¢æˆ·ç«¯ç»™äºˆè¾ƒä½è¯„åˆ†

        history = self.client_contribution_history[client_id]

        # å‚ä¸è½®æ•°
        participation_count = history.get('participation_count', 0)
        participation_score = min(1.0, participation_count / 20)

        # å‚ä¸å¯†åº¦
        recent_window = min(10, round_idx)
        recent_participation = len([r for r in history.get('participated_rounds', [])
                                    if r >= round_idx - recent_window])
        participation_density = recent_participation / recent_window if recent_window > 0 else 0

        # è¿ç»­æ€§è¯„åˆ†
        continuity_score = self._calculate_continuity_score(history.get('participated_rounds', []))

        stability = (
                0.4 * participation_score +
                0.4 * participation_density +
                0.2 * continuity_score
        )

        return stability

    def _calculate_continuity_score(self, participated_rounds):
        """è®¡ç®—å‚ä¸è¿ç»­æ€§"""
        if len(participated_rounds) < 2:
            return 0.0

        intervals = np.diff(sorted(participated_rounds))
        if len(intervals) == 0:
            return 1.0

        avg_interval = np.mean(intervals)
        interval_variance = np.var(intervals)

        if avg_interval == 0:
            return 1.0

        continuity = 1.0 / (1.0 + interval_variance / avg_interval)
        return continuity

    def _calculate_gradient_quality(self, client_id):
        """è®¡ç®—æ¢¯åº¦è´¡çŒ®è´¨é‡"""
        if client_id not in self.client_contribution_history:
            return 0.5

        history = self.client_contribution_history[client_id]

        # æŸå¤±æ”¹å–„ç¨³å®šæ€§
        loss_improvements = history.get('loss_improvements', [])
        if len(loss_improvements) < 2:
            return 0.5

        recent_improvements = loss_improvements[-5:]
        avg_improvement = np.mean([max(0, imp) for imp in recent_improvements])
        improvement_stability = 1.0 / (1.0 + np.std(recent_improvements))

        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality_score = min(1.0, avg_improvement * 10) * improvement_stability

        return quality_score

    def _calculate_cooperation_consistency(self, client_id):
        """è®¡ç®—åä½œä¸€è‡´æ€§"""
        if client_id not in self.client_contribution_history:
            return 0.5

        history = self.client_contribution_history[client_id]
        client_losses = history.get('losses', [])

        if len(client_losses) < 3:
            return 0.5

        # ä¸å…¨å±€è¶‹åŠ¿ä¸€è‡´æ€§
        global_losses = getattr(self.server_instance, 'train_history', {}).get('global_loss', [])

        if len(global_losses) < 3:
            return 0.5

        # è®¡ç®—è¶‹åŠ¿ç›¸å…³æ€§
        min_len = min(len(client_losses), len(global_losses), 5)
        client_trend = np.diff(client_losses[-min_len:])
        global_trend = np.diff(global_losses[-min_len:])

        if len(client_trend) == 0 or len(global_trend) == 0:
            return 0.5

        correlation = np.corrcoef(client_trend, global_trend)[0, 1]
        correlation = 0 if np.isnan(correlation) else correlation

        consistency = (correlation + 1) / 2  # æ˜ å°„åˆ°[0,1]
        return consistency

    def constrain_weights(self, fused_weights, reference_weights, round_idx):
        """æƒé‡çº¦æŸä¿æŠ¤æœºåˆ¶"""
        if reference_weights is None:
            return fused_weights, {'triggered': False, 'deviation': 0}

        # è®¡ç®—çº¦æŸé˜ˆå€¼
        constraint_threshold = self._get_dynamic_constraint_threshold(round_idx)

        # è®¡ç®—åç¦»ç¨‹åº¦
        weight_deviation = np.linalg.norm(fused_weights - reference_weights)

        constraint_info = {
            'deviation': weight_deviation,
            'threshold': constraint_threshold,
            'triggered': weight_deviation > constraint_threshold
        }

        if constraint_info['triggered']:
            # åº”ç”¨çº¦æŸ
            direction = (fused_weights - reference_weights) / weight_deviation
            constrained_weights = reference_weights + constraint_threshold * direction

            # é‡æ–°æ ‡å‡†åŒ–
            constrained_weights = constrained_weights / np.sum(constrained_weights)
            constrained_weights = np.maximum(constrained_weights, 0.0)
            constrained_weights = constrained_weights / np.sum(constrained_weights)

            constraint_info['adjustment'] = np.linalg.norm(constrained_weights - fused_weights)
            return constrained_weights, constraint_info

        return fused_weights, constraint_info

    def _get_dynamic_constraint_threshold(self, round_idx):
        """åŠ¨æ€çº¦æŸé˜ˆå€¼"""
        # åŸºç¡€çº¦æŸå¼ºåº¦
        base_strength = self.base_constraint

        # æ—¶é—´å› å­ï¼šåæœŸçº¦æŸæ›´ä¸¥æ ¼
        time_factor = 0.5 + 0.5 * (round_idx / 100)

        # æ³¢åŠ¨å› å­ï¼šåŸºäºæœ€è¿‘æ€§èƒ½æ³¢åŠ¨
        volatility_factor = 1.0
        if len(self.round_history) >= 3:
            recent_losses = [h.get('avg_loss', 1.0) for h in self.round_history[-3:]]
            volatility = np.std(recent_losses) / (np.mean(recent_losses) + 1e-8)
            volatility_factor = 1.0 + min(1.0, volatility)

        threshold = base_strength * time_factor * volatility_factor
        return np.clip(threshold, 0.1, 0.5)

    def _update_client_history(self, client_stats, final_weights, round_idx):
        """æ›´æ–°å®¢æˆ·ç«¯å†å²è®°å½•"""
        for i, stats in enumerate(client_stats):
            client_id = str(stats.client_id)

            if client_id not in self.client_contribution_history:
                self.client_contribution_history[client_id] = {
                    'participation_count': 0,
                    'participated_rounds': [],
                    'losses': [],
                    'loss_improvements': [],
                    'weights_received': []
                }

            history = self.client_contribution_history[client_id]

            # æ›´æ–°å‚ä¸è®°å½•
            history['participation_count'] += 1
            history['participated_rounds'].append(round_idx)

            # æ›´æ–°æŸå¤±è®°å½•
            current_loss = float(stats.loss)
            history['losses'].append(current_loss)

            # è®¡ç®—æŸå¤±æ”¹å–„
            if len(history['losses']) >= 2:
                improvement = history['losses'][-2] - history['losses'][-1]
                history['loss_improvements'].append(improvement)

            # è®°å½•è·å¾—çš„æƒé‡
            history['weights_received'].append(float(final_weights[i]))

            # ä¿æŒå†å²è®°å½•é•¿åº¦
            max_history = 20
            for key in ['losses', 'loss_improvements', 'weights_received']:
                if len(history[key]) > max_history:
                    history[key] = history[key][-max_history:]

            if len(history['participated_rounds']) > max_history:
                history['participated_rounds'] = history['participated_rounds'][-max_history:]

    def _log_enhanced_aggregation_info(self, round_idx, alpha, llm_weights,
                                       safe_weights, final_weights, constraint_info):
        """è¾“å‡ºå¢å¼ºç‰ˆèšåˆä¿¡æ¯"""
        print(f"\nğŸ”„ è½®æ¬¡ {round_idx} - å¢å¼ºç‰ˆåŠ¨æ€æƒé‡èšåˆ")
        print(f"   ğŸ“Š è¡°å‡å› å­ Î± = {alpha:.3f}")
        print(f"   ğŸ¤– LLMæƒé‡: {[f'{w:.3f}' for w in llm_weights]}")
        print(f"   ğŸ›¡ï¸  å®‰å…¨æƒé‡: {[f'{w:.3f}' for w in safe_weights]}")
        print(f"   âš–ï¸  æœ€ç»ˆæƒé‡: {[f'{w:.3f}' for w in final_weights]}")

        if constraint_info['triggered']:
            print(f"   âš ï¸  çº¦æŸè§¦å‘: åç¦» {constraint_info['deviation']:.3f} > é˜ˆå€¼ {constraint_info['threshold']:.3f}")
            print(f"   ğŸ”§ æƒé‡è°ƒæ•´: {constraint_info.get('adjustment', 0):.3f}")
        else:
            print(f"   âœ… çº¦æŸæ»¡è¶³: åç¦» {constraint_info['deviation']:.3f} â‰¤ é˜ˆå€¼ {constraint_info['threshold']:.3f}")

        # æƒé‡åˆ†å¸ƒåˆ†æ
        max_weight = max(final_weights)
        min_weight = min(final_weights)
        weight_entropy = -sum(w * np.log(w + 1e-10) for w in final_weights)
        max_entropy = np.log(len(final_weights))
        diversity_ratio = weight_entropy / max_entropy

        print(f"   ğŸ“ˆ æƒé‡ç»Ÿè®¡: æœ€å¤§={max_weight:.3f}, æœ€å°={min_weight:.3f}, å¤šæ ·æ€§={diversity_ratio:.1%}")