# -*- coding: utf-8 -*-
"""
è”é‚¦å­¦ä¹ è®­ç»ƒä¸»è„šæœ¬
"""

import os
import sys
import torch
import random
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from models.TimeLLM import Model
from dataset.data_loader import get_federated_data
from utils.config import get_args
from federated.client import FederatedClient
from federated.server import FederatedServer
from torch.utils.data import DataLoader
from utils.utils import assign_model_to_client, cleanup_client_model


class ModelConfig:
    """TimeLLMæ¨¡å‹é…ç½®ç±» - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, args):
        self.task_name = 'long_term_forecast'
        self.seq_len = args.seq_len
        self.pred_len = args.pred_len
        self.enc_in = 1
        self.d_model = args.d_model
        self.n_heads = args.n_heads
        self.d_ff = args.d_model * 4

        # LLMé…ç½® - æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®
        self.llm_model = getattr(args, 'llm_model_name', 'Qwen3')

        # åˆå§‹è®¾ç½®ï¼Œä¼šåœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶æ›´æ–°ä¸ºå®é™…å€¼
        if self.llm_model == 'Qwen3':
            self.llm_dim = 1024  # Qwen3-0.6Bçš„éšè—ç»´åº¦ï¼ˆåˆå§‹å€¼ï¼‰
            self.llm_layers = min(6, getattr(args, 'llm_layers', 6))
        elif self.llm_model == 'GPT2':
            self.llm_dim = 768
            self.llm_layers = 6
        else:
            self.llm_dim = 768  # é»˜è®¤å€¼
            self.llm_layers = 6

        # è¡¥ä¸é…ç½®
        self.patch_len = 16
        self.stride = 8

        self.dropout = args.dropout
        self.prompt_domain = True
        self.content = "The dataset records the wireless traffic of a certain base station"

        # LoRAé…ç½®
        self.use_lora = getattr(args, 'use_lora', False)
        if self.use_lora:
            self.lora_rank = getattr(args, 'lora_rank', 8)
            self.lora_alpha = getattr(args, 'lora_alpha', 16)
            self.lora_dropout = getattr(args, 'lora_dropout', 0.1)
            self.lora_target_modules = getattr(args, 'lora_target_modules',
                                               ["q_proj", "k_proj", "v_proj", "o_proj"])


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)


def create_client_data_loaders(federated_data, args):
    """ä¸ºæ‰€æœ‰å®¢æˆ·ç«¯åˆ›å»ºæ•°æ®åŠ è½½å™¨ - åŒ…å«è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†"""
    client_loaders = {}

    for client_id, client_data in federated_data['clients'].items():
        sequences = client_data['sequences']
        client_loaders[client_id] = {}

        # è®­ç»ƒé›†
        if 'train' in sequences:
            X_train = torch.FloatTensor(sequences['train']['history'])
            y_train = torch.FloatTensor(sequences['train']['target'])
            train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
            client_loaders[client_id]['train'] = DataLoader(
                train_dataset, batch_size=args.local_bs, shuffle=True
            )
            client_loaders[client_id]['num_samples'] = len(train_dataset)

        # éªŒè¯é›†
        if 'val' in sequences:
            X_val = torch.FloatTensor(sequences['val']['history'])
            y_val = torch.FloatTensor(sequences['val']['target'])
            val_dataset = torch.utils.data.TensorDataset(X_val, y_val)
            client_loaders[client_id]['val'] = DataLoader(
                val_dataset, batch_size=args.local_bs, shuffle=False
            )

        # æµ‹è¯•é›†
        if 'test' in sequences:
            X_test = torch.FloatTensor(sequences['test']['history'])
            y_test = torch.FloatTensor(sequences['test']['target'])
            test_dataset = torch.utils.data.TensorDataset(X_test, y_test)
            client_loaders[client_id]['test'] = DataLoader(
                test_dataset, batch_size=args.local_bs, shuffle=False
            )

    return client_loaders


def create_federated_clients(federated_data, client_loaders, args):
    """åˆ›å»ºè”é‚¦å®¢æˆ·ç«¯ - æ”¯æŒå¤šç§æ•°æ®åŠ è½½å™¨å’ŒçœŸå®æ•°æ®"""
    clients = []

    for client_id in federated_data['clients'].keys():
        # è·å–è¯¥å®¢æˆ·ç«¯çš„çœŸå®åæ ‡å’Œæµé‡ç»Ÿè®¡ä¿¡æ¯
        client_data = federated_data['clients'][client_id]
        coordinates = client_data['coordinates']
        original_traffic_stats = client_data['original_traffic_stats']

        # åˆ›å»ºå®¢æˆ·ç«¯ï¼Œä¼ é€’çœŸå®çš„åæ ‡å’Œæµé‡æ•°æ®
        client = FederatedClient(
            client_id=client_id,
            model=None,  # æš‚æ—¶ä¸åˆ†é…æ¨¡å‹
            data_loader=client_loaders[client_id]['train'],  # ä¸»è¦è®­ç»ƒæ•°æ®
            args=args,
            coordinates=coordinates,  # çœŸå®åæ ‡
            original_traffic_stats=original_traffic_stats  # çœŸå®æµé‡ç»Ÿè®¡
        )

        # æ·»åŠ éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
        if 'val' in client_loaders[client_id]:
            client.val_loader = client_loaders[client_id]['val']
        if 'test' in client_loaders[client_id]:
            client.test_loader = client_loaders[client_id]['test']

        clients.append(client)

    return clients


def main():
    """ä¸»å‡½æ•°"""
    # è·å–å‚æ•°
    args = get_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    print("=== è”é‚¦å­¦ä¹ è®­ç»ƒ ===")
    print(f"è®¾å¤‡: {args.device}")
    print(f"å®¢æˆ·ç«¯æ•°é‡: {args.num_clients}")
    print(f"å‚ä¸æ¯”ä¾‹: {args.frac}")
    print(f"æ€»è½®æ•°: {args.rounds}")
    print(f"æœ¬åœ°è®­ç»ƒè½®æ•°: {args.local_epochs}")
    print(f"èšåˆç®—æ³•: {args.aggregation}")

    # åŠ è½½è”é‚¦æ•°æ®
    print("\nåŠ è½½è”é‚¦æ•°æ®...")
    federated_data, _ = get_federated_data(args)

    # éªŒè¯å’Œå±•ç¤ºçœŸå®æ•°æ®ï¼ˆæ–°å¢ï¼‰
    from utils.utils import validate_real_data, print_real_data_summary

    if validate_real_data(federated_data):
        print_real_data_summary(federated_data)
    else:
        print("æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        return

    # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®åŠ è½½å™¨
    print("\nåˆ›å»ºå®¢æˆ·ç«¯æ•°æ®åŠ è½½å™¨...")
    client_loaders = create_client_data_loaders(federated_data, args)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    print("åˆå§‹åŒ–å…¨å±€æ¨¡å‹...")
    global_model = Model(ModelConfig(args)).to(args.device)

    # åˆ›å»ºè”é‚¦å®¢æˆ·ç«¯ï¼ˆç°åœ¨ä¼šä¼ é€’çœŸå®æ•°æ®ï¼‰
    print("åˆ›å»ºè”é‚¦å®¢æˆ·ç«¯...")
    clients = create_federated_clients(federated_data, client_loaders, args)
    print(f"æˆåŠŸåˆ›å»º {len(clients)} ä¸ªå®¢æˆ·ç«¯")

    # æ‰“å°å®¢æˆ·ç«¯çœŸå®æ•°æ®æ ·æœ¬ï¼ˆæ–°å¢ï¼‰
    print("\nå®¢æˆ·ç«¯çœŸå®æ•°æ®æ ·æœ¬:")
    sample_client = clients[0]
    print(f"  åŸºç«™ {sample_client.client_id}:")
    print(f"    åæ ‡: ({sample_client.coordinates['lng']:.3f}, {sample_client.coordinates['lat']:.3f})")
    traffic_stats = sample_client.get_real_traffic_stats()
    print(f"    æµé‡ç»Ÿè®¡: å‡å€¼={traffic_stats['mean']:.1f}, è¶‹åŠ¿={traffic_stats['trend']}")
    print(f"    å˜å¼‚ç³»æ•°: {traffic_stats.get('coefficient_of_variation', 0):.3f}")

    # åˆ›å»ºè”é‚¦æœåŠ¡å™¨
    server = FederatedServer(global_model, args)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
    start_round = 1
    best_val_loss = float('inf')

    if args.resume:
        try:
            start_round, best_val_loss = server.load_checkpoint(args.resume)
            print(f"ä»è½®æ¬¡ {start_round} æ¢å¤è®­ç»ƒï¼Œå½“å‰æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        except Exception as e:
            print(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            start_round = 1
            best_val_loss = float('inf')

    # å¼€å§‹è”é‚¦è®­ç»ƒ
    print(f"\nå¼€å§‹è”é‚¦è®­ç»ƒ (ä»è½®æ¬¡ {start_round} åˆ° {args.rounds})...")

    for round_idx in range(start_round, args.rounds + 1):
        print(f"\n{'=' * 50}")
        print(f"è”é‚¦å­¦ä¹ è½®æ¬¡: {round_idx}/{args.rounds}")

        # æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024 ** 3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024 ** 3  # GB
            print(f"GPUæ˜¾å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")

        # æ‰§è¡Œä¸€è½®è”é‚¦å­¦ä¹ 
        round_results = server.federated_round(clients, round_idx)

        # è¾“å‡ºæœ¬è½®ç»“æœï¼ˆå¢å¼ºç‰ˆï¼‰
        avg_loss = round_results['avg_client_loss']
        print(f"æœ¬è½®å¹³å‡å®¢æˆ·ç«¯æŸå¤±: {avg_loss:.6f}")

        # å¦‚æœæœ‰éªŒè¯æŸå¤±ï¼Œä¹Ÿè¾“å‡º
        if 'val_loss' in round_results:
            print(f"æœ¬è½®éªŒè¯é›†æŸå¤±: {round_results['val_loss']:.6f}")

        # æ¯éš”ä¸€å®šè½®æ•°è¿›è¡Œå…¨å±€è¯„ä¼°
        if round_idx % args.eval_every == 0:
            print("è¿›è¡Œå…¨å±€æ¨¡å‹è¯„ä¼°...")

            eval_clients = clients[:min(10, len(clients))]  # å¢åŠ è¯„ä¼°å®¢æˆ·ç«¯æ•°é‡

            # éªŒè¯é›†è¯„ä¼°
            if hasattr(eval_clients[0], 'val_loader'):
                val_loss, val_client_losses = server.evaluate_global_model_detailed(eval_clients, 'val')
                server.train_history['global_loss'].append(val_loss)
                print(f"å…¨å±€éªŒè¯æŸå¤±: {val_loss:.6f}")

                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                if args.save_best_model and val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_path = f"{args.save_dir}/best_model.pth"
                    server.save_best_model(best_model_path, val_loss, round_idx)
                    print(f"ğŸ¯ å‘ç°æ›´ä¼˜æ¨¡å‹ï¼éªŒè¯æŸå¤±: {val_loss:.6f}")

            # å¦‚æœä¹Ÿæƒ³çœ‹æµ‹è¯•é›†è¡¨ç°ï¼ˆå¯é€‰ï¼Œä½†ä¸ç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰
            if hasattr(eval_clients[0], 'test_loader') and round_idx % (args.eval_every * 2) == 0:
                test_loss, _ = server.evaluate_global_model_detailed(eval_clients, 'test')
                print(f"å½“å‰æµ‹è¯•æŸå¤±: {test_loss:.6f} (ä»…ä¾›å‚è€ƒ)")

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if args.save_checkpoint and round_idx % args.checkpoint_interval == 0:
            checkpoint_path = f"{args.save_dir}/checkpoint_round_{round_idx}.pth"
            server.save_checkpoint(checkpoint_path, round_idx, best_val_loss)

    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
    if args.save_checkpoint:
        final_checkpoint_path = f"{args.save_dir}/final_checkpoint.pth"
        server.save_checkpoint(final_checkpoint_path, args.rounds, best_val_loss)
        print(f"æœ€ç»ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")

    print("\nè”é‚¦è®­ç»ƒå®Œæˆ!")

    # æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°ï¼ˆå¦‚æœå·²å®ç°ï¼‰
    if hasattr(server, 'final_test_evaluation'):
        final_results = server.final_test_evaluation(clients)

    # è¾“å‡ºè®­ç»ƒæ‘˜è¦
    train_history = server.get_train_history()

    print(f"\n{'=' * 60}")
    print("è®­ç»ƒæ‘˜è¦")
    print(f"{'=' * 60}")

    # è®­ç»ƒæŸå¤±
    final_client_loss = train_history['client_losses'][-1] if train_history['client_losses'] else float('inf')
    print(f"æœ€ç»ˆå¹³å‡å®¢æˆ·ç«¯è®­ç»ƒæŸå¤±: {final_client_loss:.6f}")

    # éªŒè¯æŸå¤±
    if 'val_losses' in train_history and train_history['val_losses']:
        final_val_loss = train_history['val_losses'][-1]
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.6f}")

    # å…¨å±€è¯„ä¼°æŸå¤±
    if train_history['global_loss']:
        best_global_loss = min(train_history['global_loss'])
        print(f"æœ€ä½³å…¨å±€æŸå¤±: {best_global_loss:.6f}")

    # æ¨¡å‹ä¿å­˜æ‘˜è¦
    if args.save_best_model:
        print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä¿¡æ¯:")
        print(f"   æœ€ä¼˜æ¨¡å‹: {args.save_dir}/best_model.pth")
        print(f"   æœ€ä¼˜éªŒè¯æŸå¤±: {best_val_loss:.6f}")

    if args.save_checkpoint:
        print(f"   æœ€ç»ˆæ£€æŸ¥ç‚¹: {args.save_dir}/final_checkpoint.pth")
        print(f"   æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”: æ¯ {args.checkpoint_interval} è½®")

    # å¦‚æœä½¿ç”¨å¤šç»´åº¦LLMèšåˆï¼Œç”Ÿæˆè¶‹åŠ¿åˆ†æ
    if args.aggregation == 'multi_dim_llm':
        print(f"\n{'=' * 60}")
        print("å®¢æˆ·ç«¯å­¦ä¹ è¶‹åŠ¿åˆ†æ")
        print(f"{'=' * 60}")

        try:
            from utils.trend_visualizer import visualize_trends
            visualize_trends(server, args.save_dir)
        except Exception as e:
            print(f"è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥: {e}")
            # è‡³å°‘æ‰“å°åŸºæœ¬çš„è¶‹åŠ¿æ‘˜è¦
            if hasattr(server, 'client_history') and server.client_history['losses']:
                print(f"å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æ•°é‡: {len(server.client_history['losses'])}")

                # æ˜¾ç¤ºå„å®¢æˆ·ç«¯çš„åŸºæœ¬è¶‹åŠ¿ä¿¡æ¯
                for client_id in server.client_history['losses'].keys():
                    trend_summary = server.get_client_trend_summary(client_id)
                    print(f"  å®¢æˆ·ç«¯ {client_id}: {trend_summary['description']} (è¯„åˆ†: {trend_summary['score']:.2f})")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    if hasattr(args, 'save_results') and args.save_results:
        import json
        results_to_save = {
            'train_history': train_history,
            'args': vars(args)
        }

        # æ·»åŠ è¶‹åŠ¿åˆ†æç»“æœ
        if args.aggregation == 'multi_dim_llm' and hasattr(server, 'client_history'):
            results_to_save['client_trends'] = {}
            for client_id in server.client_history['losses'].keys():
                trend_summary = server.get_client_trend_summary(client_id)
                results_to_save['client_trends'][client_id] = trend_summary

        # æ·»åŠ æœ€ç»ˆæµ‹è¯•ç»“æœ
        if 'final_results' in locals():
            results_to_save['final_test_results'] = final_results

        with open(f"{args.save_dir}/training_results.json", 'w') as f:
            json.dump(results_to_save, f, indent=2, default=str)
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {args.save_dir}/training_results.json")


if __name__ == "__main__":
    main()